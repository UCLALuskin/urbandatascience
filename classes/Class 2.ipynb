{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fce03f",
   "metadata": {},
   "source": [
    "## Module 2 Class activities\n",
    "This notebook is a starting point for the exercises and activities that we'll do in class.\n",
    "\n",
    "Before you attempt any of these activities, make sure to watch the video lectures for this module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092609b",
   "metadata": {},
   "source": [
    "### Scraping permit data\n",
    "Here's the code that we saw in the video lecture that queries the City of Seattle permit website, gets a dataframe of permits (including the URL), and then digs down further into that permit-specific URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89222500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the permit data from the API\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://data.seattle.gov/resource/ht3q-kdvx.json' # copied and pasted from the webpage\n",
    "r = requests.get(url)\n",
    "df = pd.DataFrame(json.loads(r.text))\n",
    "\n",
    "df = df.head(5) # get the first 5 rows, so we don't overload the city's website.\n",
    "\n",
    "# get an example link\n",
    "permiturl = df.loc[0,'link']['url']\n",
    "print(permiturl)\n",
    "\n",
    "# request that page and get the soup object\n",
    "r = requests.get(permiturl)\n",
    "soup = BeautifulSoup(r.text)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59022053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we wrote this code to extract the project description \n",
    "links = soup.find_all('td')\n",
    "for link in links:\n",
    "    if 'Project Description' in link.text: \n",
    "        sublinks = link.find_all('td')\n",
    "        description = sublinks[1].text\n",
    "        # once we find a description, we exit\n",
    "        break\n",
    "    \n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d730d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> If you look at the example, there is a <strong>Legal Description</strong> section. Extract that to a variable and print it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041fefc9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Now turn that into a function that you can apply to each row of your dataframe. Add a new column, <strong>legal_description</strong>, to your dataframe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774e339",
   "metadata": {},
   "source": [
    "### Fixing errors\n",
    "We'll do more scraping in just a moment. But first, let's do some examples of how to interpret an error message, and fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7c600",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Each of the cells below will generate an error. Look at the error message and see if you can figure out how to fix it. (Don't Google it until you try to figure it out based on the error message.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dad38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the housingunitsremoved and housingunitsadded give useful information\n",
    "# let's create a new column with netunits\n",
    "df['netunits'] = df.housingunitsadded - df.housingunitsremoved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the address of the first row\n",
    "print('Address of first row is {}. Permit type is {}'.format(df.iloc[0].originaladdress1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6dfde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the number of housing units to integers\n",
    "# and then summarize\n",
    "\n",
    "df['unitsadded_numeric'] = df.housingunitsadded.astype(int)\n",
    "df.unitsadded_numeric.describe("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e25862",
   "metadata": {},
   "source": [
    "### Scraping craigslist\n",
    "\n",
    "In the lecture, we saw how to scrape the main page (the list of posts).\n",
    "\n",
    "What if you want to get more information about (say) a particular apartment?\n",
    "\n",
    "Go to the [craigslist housing page](https://losangeles.craigslist.org/search/apa#search=1~gallery~0~0) and copy the link for one of the listings. It should look something like this:\n",
    "https://losangeles.craigslist.org/lgb/apa/d/long-beach-home-for-rent/7597309102.html\n",
    "\n",
    "(It's fine to copy and paste the URL for now. A second step would be to loop over the URLs from the dataframe of postings that we created in the video lecture, but in class, we'll just focus on one example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3f6d0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> For this URL, use requests to get the content of the post. (No need to create a soup object yet.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8705a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# your code here\n",
    "# put the output of the request in a variable called r\n",
    "# so you can access the content like this\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e3794",
   "metadata": {},
   "source": [
    "Now let's extract more information from the page. We have a couple of strategies here. First, we could skip trying to parse the page with `BeautifulSoup`, and just see if particular bits of text are present.\n",
    "\n",
    "For example, what transportation modes does the post emphasize? Do they mention Section 8 vouchers? Some of this might be exploratoryâ€”we can see what type of language is included, and then parse in a more structured way (e.g. distinguishing between \"No Section 8\" and \"Section 8 welcome\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5bdf82",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Write a function that will return True if Section 8 is mentioned, otherwise False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb944b5",
   "metadata": {},
   "source": [
    "*Hint*: the `in` operator is a simple way to do this. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'plan' in 'urban planning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f97024",
   "metadata": {},
   "outputs": [],
   "source": [
    "'plan' in 'Urban Planning' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806158ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to return Section 8 information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707bacd1",
   "metadata": {},
   "source": [
    "Most of the post is free-form text. So there's not going to be much value added by `BeautifulSoup`.\n",
    "\n",
    "The exceptions are (i) parking, and (ii) the geographic coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30412ae2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Write a function that will return True if the apartment has no parking, and also returns the lat/lon of the apartment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a497c9",
   "metadata": {},
   "source": [
    "*Hint*: First, create a `soup` object. Then, look and see what tag and class encloses this information. Then, you can experiment with `find` and `find_all` with this tag and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7baf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731adec-f656-4816-9763-81366c3c0301",
   "metadata": {},
   "source": [
    "Now you've written this code, a next step would be to package it in a function that you can apply to all the URLs in your dataframe of posts (like the one we created in the video lecture). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2818a26",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>What you should have learned</h3>\n",
    "<ul>\n",
    "  <li>Gain confidence in experimenting with code - exploring different objects, writing functions, and so on</li>\n",
    "  <li>Learn how to extract information from a scraped webpage - how to do the detective work.</li>\n",
    "  <li>Gain confidence in debugging errors.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
