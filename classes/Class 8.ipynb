{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fce03f",
   "metadata": {},
   "source": [
    "## Module 8 Class activities\n",
    "This notebook is a starting point for the exercises and activities that we'll do in class. We'll parse some PDFs, and then implement some simple \"bag of words\" models.\n",
    "\n",
    "Our running example: the [City of Palm Springs General Plan update](https://www.psgeneralplan.com).\n",
    "\n",
    "Before you attempt any of these activities, make sure to watch the video lectures for this module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092609b",
   "metadata": {},
   "source": [
    "### Reading and cleaning PDFs\n",
    "\n",
    "Let's read in [this PDF of public comments](https://www.psgeneralplan.com/_files/ugd/89af76_0b8c3cd9a25140f4a9791570af8d6ba0.pdf). It's in the `data/` folder in your GitHub repository.\n",
    "\n",
    "I suggest using `pdfminer` (as in Lecture 17), but you could try other PDF parsing libraries if you like. If you didn't install it already while watching the video lectures, you'll need to do that first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30faed6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<strong>Exercise:</strong> Read in the public comments to a string.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549f5a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<strong>Exercise:</strong> Inspect the string. Does it look usable? Write down what you think you'd need to do to clean it up. \n",
    "    \n",
    "Explain your reasoning to your neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab226a",
   "metadata": {},
   "source": [
    "Write down your thoughts here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4113c49",
   "metadata": {},
   "source": [
    "You may have come up with better ideas (implement them!).\n",
    "\n",
    "First, note that the first 9 pages are survey responses - not comments. The first \"real\" comment is \"It doesn't speak about the people...\"\n",
    "\n",
    "You can exclude those in a couple of ways. We could have ignored the first 8 pages when loading in via `pdfminer`. Or we can search for the text, \"It doesn't speak about the people,\" and remove all of the earlier part of the string as follows:\n",
    "* `txt.find(\"It doesn't\")` will give you the location of where that comment starts\n",
    "* `txt = txt[txt.find(\"It doesn't\"):]` will give you a new string that starts at that location.\n",
    "\n",
    "Once we've done that, we can convert all tabs, newlines, and multiple spaces to a single space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f6ff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "my_text = 'This is the part of the string I want to exclude. We can start here.'\n",
    "print(my_text.find('We can start'))\n",
    "print(my_text[my_text.find('We can start'):])\n",
    "\n",
    "# this is the same\n",
    "print(my_text[50:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89523f4a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Clean up the comments, by (i) removing the first 8 pages, (ii) removing the characters that are not letters or spaces, and (iii) replacing whitespace (e.g. newlines, tabs, and multiple spaces) with a single space.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9519f3",
   "metadata": {},
   "source": [
    "*Hints:*\n",
    "* You'll need `regex`\n",
    "* Start with the first comment - write a regex that will clean up the first element in your list\n",
    "* Once you've figured that out, apply it to everything in your list with a loop or list comprehension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b5b4f",
   "metadata": {},
   "source": [
    "### Word counts\n",
    "Now we have our cleaned up string. Let's look at a bag of words model - basically, what are the most frequent words. \n",
    "\n",
    "First, we need to split our string into a list of words, and remove the stopwords. Hint: Think about case too!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24327cee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Create a list of words, removing stopwords.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621f6ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582ec3ad",
   "metadata": {},
   "source": [
    "In the lecture video, we wrote our own custom word count function. But `nltk` actually has that built in. It does counts of individual words, but also of N-grams. For example, a bigram (N-gram of 2) could be \"Palm Springs\" or \"affordable housing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcae153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "from nltk import ngrams, FreqDist\n",
    "sample_words = ['a', 'list','of','words','some', 'repeat', 'and','repeat','themselves', 'repeat','themselves']\n",
    "FreqDist(ngrams(sample_words, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97414dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example bigrams\n",
    "FreqDist(ngrams(sample_words, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80163d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also plot the frequencies\n",
    "FreqDist(ngrams(sample_words, 1)).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c947fed",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Plot the frequencies of words (and bigrams) in your own wordlist. Experiment with the plot arguments, and what you can do with the object returned by <strong>FreqDist</strong>. Based on your results, you might want to add more words to your list of stopwords.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5215f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7850b937",
   "metadata": {},
   "source": [
    "## Extensions to bags of words\n",
    "One potential use of these \"bags of words\" models is to look at differences across space or across time. For example, do building or land use permits change over time - when does \"add garage\" become less common, and \"add ADU\" more common. \n",
    "\n",
    "Or we could look at how word frequency changes across space or cities.\n",
    "\n",
    "Let's try this with the San Francisco Board of Supervisors meetings. [This webpage](http://sanfrancisco.granicus.com/ViewPublisher.php?view_id=10) gives the archived transcripts (\"caption notes\").\n",
    "\n",
    "You could scrape all of the URLs (that's a good exercise!), but for now, just manually create a list with a half dozen or so of the caption notes.\n",
    "\n",
    "Write a function that for a given URL:\n",
    "* Gets the text (use `requests`)\n",
    "* Cleans the text\n",
    "* Gets the word counts\n",
    "\n",
    "Then, think about how you might chart the results over time.\n",
    "\n",
    "This is an open-ended prompt, so spend some time thinking through the steps conceptually, even if you don't get far in implementing it. For example, how will you organize the text of each documents and the counts? In a list? A dataframe? Will you loop through each URL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae74ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2818a26",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>You should now be able to:</h3>\n",
    "<ul>\n",
    "  <li>Read a PDF into a Python string</li>\n",
    "  <li>Clean the string</li>\n",
    "  <li>Count words and n-grams, and plot the results</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
