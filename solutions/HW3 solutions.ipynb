{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddcce680",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b1217afff8bc58c2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Homework 3: Machine learning\n",
    "\n",
    "In this assignment, we'll practice the classification skills from machine learning. We'll use the precinct-level voting data to predict support for Prop 21 (rent control) on the 2020 ballot. For example, we might expect the share of renters to be an important predictor.\n",
    "\n",
    "We'll also review joins as we prepare the data.\n",
    "\n",
    "Start by loading the 2020 elections results from LA County into a `pandas` dataframe, `voteDf`. (This is exactly the same data as we will use next week in the clustering lectures; I put another copy of the data file in the assignment GitHub folder to make things easier.)\n",
    "\n",
    "### Policy on ChatGPT / AI\n",
    "\n",
    "This is the same as in HWs 1 and 2. Please review those guidelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce9f07",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4f721042cd05425",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please help me grade by observing the following:\n",
    " \n",
    "* Do not rename this notebook (that messes up the autograder)\n",
    "* Do not include large sections of output (that makes it hard to find your code). For example, use `df.head()` to show the first few rows, rather than printing an entire dataframe. The same goes for printing long strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f8fa1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ff091f01ca8b647",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "voteDf = 999 # replace with your dataframe\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "fn = 'c037_g20_sov_data_by_g20_srprec.csv'\n",
    "voteDf = pd.read_csv(fn)\n",
    "### END SOLUTION\n",
    "print(len(voteDf))\n",
    "print(voteDf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55c4df1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-82c4c318b68ac93b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograding tests - do not edit\n",
    "assert len(voteDf) == 4313\n",
    "assert isinstance(voteDf, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4ff214",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8e73935274cd6bac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To do some prediction, we'll want to add variables from (say) the census or other sources.\n",
    "For that, we need the lookup file that matches precincts to census blocks and tracts. [You can find it here](https://statewidedatabase.org/d10/g20_geo_conv.html), or just use the file `c037_g20_sr_blk_map.csv` in your GitHub repository. (Note that there are several types of precincts; the ones that we are using here are called `srprec`.) \n",
    "\n",
    "Each precinct intersects with many census blocks. The `pctsrprec` column tells you how much of the precinct lies within that block. For example, in the first few rows of `c037_g20_sr_blk_map.csv`, you'll see 49 different rows for precinct `0050003A`, each matching to a different census block, with the `pctsrprec` column adding up to 100.\n",
    "\n",
    "Our aim is to create a new dataframe with the vote counts (for all of the propositions and other races) aggregated to census tract. This is a multi-stage process, so let's do this step by step.\n",
    "\n",
    "In this step, you should:\n",
    "- load in the lookup data into a new dataframe, `lookupDf`\n",
    "- join the voting dataframe to the lookup dataframe using `srprec`, to create a new dataframe called `joinDf`. This is a 1:many join, since there are many census blocks per precinct. Do an inner join, as the Null values are not going to be useful to us. (In other words, throw away any lookups that don't match a precinct.)\n",
    "- make sure that `srprec` is the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82282498",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7ed8bd92abd47eh",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lookupDf = 999 # your code here\n",
    "joinDf = 999\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "fn2 = 'c037_g20_sr_blk_map.csv'\n",
    "\n",
    "lookupDf = pd.read_csv(fn2)\n",
    "\n",
    "# inner join to our voting data\n",
    "joinDf = voteDf.set_index('srprec').join(lookupDf.set_index('srprec'), how='inner')\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f2f2f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-286dda81152bd482",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(lookupDf))\n",
    "print(len(joinDf))\n",
    "print(joinDf.county.count())\n",
    "print(joinDf.TOTREG.sum())\n",
    "assert joinDf.index.name=='srprec'\n",
    "assert len(lookupDf)==77704\n",
    "assert len(joinDf)==77703\n",
    "assert joinDf.county.count()==77703\n",
    "assert joinDf.TOTREG.sum()==168427815"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6befe423",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7bcea0e4d8ff0da9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's calculate vote shares on Prop 21 and in the presidential race for each census tract. \n",
    "\n",
    "This is slightly tricky, because your data frame `joinDf` will have multiple rows per tract (because the precinct geography does not match the census geography). For example, the following code shows you which precincts intersect with tract 119342. \n",
    "\n",
    "13.65% of the first precinct listed, `9004204A`, is in tract 119342."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15750cae",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-64a15291025f017a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "joinDf[joinDf.tract==119342][['tract','pctsrprec']].sort_index(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7067641",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-929fd94be63a3489",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "So to aggregate to tracts, you should:\n",
    "- for each relevant column, multiply the number of votes by `pctsrprec`, and divide by 100 (because `pctsrprec` is a percentage, not a fraction)\n",
    "- group by census tract and sum those relevant columns, to create a new dataframe called `tractVotes`. It should have columns `PR_21_N`, `PR_21_Y`, `PRSDEM01`, `PRSREP01`, etc.\n",
    "\n",
    "This will give us our estimate of votes at the tract level.\n",
    "\n",
    "*Hint*: You can pass multiple columns to `groupby`. E.g. `df.groupby('groupcol')[['col1','col2','col3']].sum()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d77819",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-863ef266d322296a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "tractVotes = 999 # your code here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "# get list of columns from the original dataframe, excluding the ones that aren't about voting\n",
    "cols = [col for col in voteDf.columns if col not in ['county','srprec','addist', 'cddist','sddist','bedist']]\n",
    "\n",
    "# number of votes per block\n",
    "for col in cols:\n",
    "    joinDf[col] = joinDf[col] * joinDf.pctsrprec / 100\n",
    "\n",
    "# aggregate to census tract\n",
    "tractVotes = joinDf.groupby('tract')[cols].sum()\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8c0926",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-d616b48580427e62",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(tractVotes))\n",
    "print(tractVotes.PR_21_Y.sum())\n",
    "\n",
    "# Autograding tests - do not edit\n",
    "assert len(tractVotes)==2338\n",
    "assert tractVotes.PR_21_Y.sum().round() == 2021487"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cbec5b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d1fa0f9a1e656b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now let's get a dataframe of some relevant census variables, using the Census Bureau API. Check back to the Week 1 example and the first homework.\n",
    "\n",
    "Create a dataframe, `censusDf`, with ACS 2019 (5 year) tract-level data for LA County, and variables for Tenure (B25003_001E, B25003_002E, B25003_003E) and median household income (B19013_001E). Add a column with the percent of renters, called `pct_renter`.\n",
    "\n",
    "Rename the median HH income column `median_hh_income`, which is more meaningful.\n",
    "\n",
    "Why use ACS 2019 rather than a more recent vintage? Well, the census tract boundaries change after each decennial census (i.e., in 2020), and the precinct-to-tract files we used above map to the pre-2020 census boundaries.\n",
    "\n",
    "As a reminder, here's the Census API [list of tables](https://api.census.gov/data/2019/acs/acs5/variables.html), and [here are examples that you can adapt](https://api.census.gov/data/2019/acs/acs5/examples.html). \n",
    "\n",
    "*Hint:* Make sure to restrict your data request by state AND county if you want to keep it to a manageable size! You shouldn't need to request an API key for a small number of queries..\n",
    "\n",
    "*Hint:* Look at your data if you get the wrong answer to median income! For example, use `censusDf.describe()`, or `censusDf.sort_values(by='med_hh_income').head()`. You might need to replace some values with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b16ca9b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-087a1df3d3eb51b0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "censusDf = 999 # your code here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "r = requests.get('https://api.census.gov/data/2019/acs/acs5?get=NAME,B25003_001E,B25003_002E,B25003_003E,B19013_001E&for=tract&in=state:06&in=county:037')\n",
    "censusdata = r.json()\n",
    "censusDf = pd.DataFrame(censusdata[1:], columns=censusdata[0])\n",
    "\n",
    "# if you tried to get calculate the percentage at this point, you'd have probably got an error because it's a string\n",
    "for col in ['B25003_001E','B25003_002E','B25003_003E','B19013_001E']:\n",
    "    censusDf[col] = censusDf[col].astype(float)\n",
    "\n",
    "censusDf['pct_renter'] = censusDf['B25003_003E'] / censusDf['B25003_001E'] * 100\n",
    "censusDf.rename(columns={'B19013_001E':'median_hh_income'}, inplace=True)\n",
    "\n",
    "# if you look at your data, you'll see there are a lot of median incomes of -666666666 \n",
    "# - probably because these tracts have no residents who are workers\n",
    "# so let's replace them with NaNs\n",
    "censusDf.loc[censusDf.median_hh_income<0,'median_hh_income'] = np.nan\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58482987",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2d674b4f0e57e6dc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print (len(censusDf))\n",
    "print (censusDf.pct_renter.mean())\n",
    "print(censusDf.median_hh_income.mean())\n",
    "\n",
    "# Autograding tests - do not edit\n",
    "assert len(censusDf) == 2346\n",
    "assert censusDf.pct_renter.mean().round() == 53\n",
    "assert censusDf.median_hh_income.mean().round()==73243"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8146828",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10df2e93cc4cd1f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Create a new dataframe, `joinedDf`, with both your voting and census data, through a left join to the voting data. \n",
    "\n",
    "*Hint*: It will be easiest to join on the `tract` column (which is your index in `tractVotes`). Since everything is in LA County, you don't need to worry about the `state` or `county` fields.\n",
    "\n",
    "*Hint*: You'll need to convert the `tract` column in `censusDf` to an integer first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd78c1fd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7826b19a96730b22",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "joinedDf = 9999\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "censusDf['tract'] = censusDf.tract.astype(int)\n",
    "joinedDf = tractVotes.join(censusDf.set_index('tract'))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49062cce",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1cd1c76f5b8264c9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(joinedDf.pct_renter.count())\n",
    "print(joinedDf.pct_renter.mean())\n",
    "\n",
    "# Autograding tests - do not edit\n",
    "assert joinedDf.pct_renter.count() == 2318\n",
    "assert joinedDf.pct_renter.mean().round() == 53"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccab3db",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0473e50f37e6f3ba",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's start with a simple random forests model with the following *x* variables:\n",
    "\n",
    "* Median HH income\n",
    "* Percent of HHs that are renters\n",
    "* Presidential vote (2-party share of Democrat voters, i.e. the percent voting for Biden vs Trump, with other candidates ignored)\n",
    "\n",
    "And the following *y* variable\n",
    "* Whether Prop 21 won (received a majority) in that census tract. This should be `True` if the Yeses got more votes than the Nos.\n",
    "\n",
    "(Yes, vote share in each tract would be better to predict rather than a binary variable - hold off on that for the challenge problem.)\n",
    "\n",
    "Create the relevant columns, `pct_biden` and `PR_21_won`, in your `joinedDf` dataframe. \n",
    "\n",
    "Then split your dataframe into a training sample (75%) and a test sample (25%). *Hint*: Drop the `NaNs` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00189327",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f292cd5bba490d15",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "X_train, X_test, y_train, y_test = 999, 999, 999, 999\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "joinedDf['pct_biden'] = joinedDf.PRSDEM01 / (joinedDf.PRSDEM01 + joinedDf.PRSREP01) * 100\n",
    "joinedDf['PR_21_won'] = joinedDf.PR_21_Y > joinedDf.PR_21_N\n",
    "xvars = ['pct_biden', 'pct_renter', 'median_hh_income']\n",
    "yvar = 'PR_21_won'\n",
    "\n",
    "df_to_fit  = joinedDf[xvars+[yvar]].dropna()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_to_fit[xvars], df_to_fit[yvar], test_size = 0.25)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37924f00",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-51a28332d08e392f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(X_train.pct_biden.mean())\n",
    "print(y_train.mean())\n",
    "\n",
    "# Autograding tests - do not edit\n",
    "assert len(X_train) == 1731\n",
    "assert len(X_train.columns) == 3\n",
    "assert len(X_test) == 578\n",
    "assert len(X_train) == len(y_train)\n",
    "assert len(X_test) == len(y_test)\n",
    "assert X_train.pct_biden.mean().round() == 74\n",
    "assert y_train.mean().round(1) == 0.6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a095a7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-da525eb9a304953d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Estimate a random forests model, and assign the predicted *y* values from your *test* sample to `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98afd7b7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-18655f54613bacb6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "# initialize the random forest regressor object\n",
    "rf = RandomForestClassifier(n_estimators = 50, random_state=1)\n",
    "\n",
    "# now fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict values\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba581a2",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-633c221bbc5dda5e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(len(y_pred))\n",
    "print(y_pred.mean())\n",
    "\n",
    "# Autograding tests - do not edit\n",
    "assert len(y_pred)==len(y_test)\n",
    "assert y_pred.mean().round(1) == 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4319c7",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-907f10cfdc92c141",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's look at some measures of fit. Plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2c942a",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c73405d47229d9c2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1fb72",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4a7868979483ed5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, plot the importance of each of the 3 predictor variables, in the same way as we did in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8efe8f",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-6db4a17c87ddf643",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# more-or-less directly taken from the docs: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "std = np.std([\n",
    "    tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "fig.tight_layout()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b82113",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0a6b521f6268911b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Comment on your interpretation of the results and the confusion matrix. What do they tell you about:\n",
    "- your predictive accuracy\n",
    "- which variables are important\n",
    "- how you might refine the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b6b202",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e2b3e80aad8c47a2",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Write a few bullet points here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61901bb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ffd460ba8c5df1e3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Challenge Problem\n",
    "Remember, you need to do at least two of these challenge problems this quarter.\n",
    "\n",
    "This challenge problem is open ended for you to take in a direction that you are most interested in. Here are some suggestions (do 1 or 2 of these):\n",
    "\n",
    "* Extend the random forests model to predict vote share on Prop 21, rather than a binary yes/no, and using additional variables. See suggestions below. \n",
    "* Use a neural network instead. How much does this improve the predictions? Use charts to compare the predictions to the random forests model.\n",
    "* Examine the geographic distribution of the predictions, through mapping the prediction errors. Where does your model perform best? Does this give you pointers as to how to improve your model?\n",
    "\n",
    "In all cases, write some brief interpretation in a markdown cell.\n",
    "\n",
    "*Predicting a continuous variable*\n",
    "\n",
    "Classification problems are typically binary or categorical - which category do you predict a given observation to fall into. In some cases, however, we might want to predict a continuous variable, such as the percentage of \"yes\" votes on Prop 21. For this we can use `RandomForestRegressor`, which works very similarly to `RandomForestClassifier`. You can follow exactly the same steps: create the `rf` object, fit the model, and predict using the test sample.\n",
    "\n",
    "How do you evaluate model performance? Since we have a continuous variable, we can't use the confusion matrix. But we can look at the absolute error (each predicted value minus the true value for each of our test precincts). I.e., `abs(y_pred-y_test)`. You can also do a scatter plot of the predicted values against the true values. The divergence from the 45 degree line is a good indication of how well the model fits."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
