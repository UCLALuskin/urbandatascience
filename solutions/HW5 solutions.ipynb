{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c49b90",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00bc092610d961a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Homework 5: Natural language processing\n",
    "\n",
    "In this assignment, we'll practice topic modeling and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9b5930",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b518a6d84f55d833",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please help me grade by observing the following:\n",
    " \n",
    "* Do not rename this notebook (that messes up the autograder)\n",
    "* Do not include large sections of output (that makes it hard to find your code). For example, use `df.head()` to show the first few rows, rather than printing an entire dataframe. The same goes for printing long strings.\n",
    "\n",
    "The same ChatGPT policy applies as in previous homework assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c852884-d0a6-476d-9a5f-13ef924f0499",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c8017f6615974fe9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We'll use a dataset of transcripts from San Francisco Planning Commission meetings from 2006-2025. In order to focus your time on the natural language processing, I did the web scraping part for you. However, I encourage you to review and tinker with the code below as a review of the earlier course material. All the transcripts are downloaded from the [Board of Supervisors meetings page](http://sanfrancisco.granicus.com/ViewPublisher.php?view_id=10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79c02908-f575-4ab1-a373-b23b4fb073c2",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-00b7867315654c49",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# if 0 is always False, so this just means this part of the code won't run \n",
    "# (you can load in the file instead of scraping it yourself)\n",
    "if 0:   \n",
    "    import requests\n",
    "    import pandas as pd \n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    r = requests.get('https://sanfrancisco.granicus.com/ViewPublisher.php?view_id=20') # Planning Commission homepage\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    links = [link.get('href') for link in soup.find_all('a')] # get all weblinks on the page\n",
    "    links = [link for link in links if 'TranscriptViewer' in link] # restrict to those that are transcripts (there are also video files)\n",
    "\n",
    "    # now loop over all the URLs, retrieve the transcript, and store it in a dictionary\n",
    "    transcripts = {}\n",
    "    for ii, link in enumerate(links):\n",
    "        clipid = link.split('clip_id=')[-1] # get the clip id, just for the dictionary key\n",
    "        if clipid in transcripts: continue\n",
    "        print ('Fetching link {} of {}: https:{}'.format(ii+1, len(links),link))\n",
    "        r = requests.get('http:'+link)\n",
    "        soup = BeautifulSoup(r.content)    \n",
    "        transcripts[clipid] = soup.text  # store the result in the dictionary with key=clip_id\n",
    "\n",
    "    # save to a zipped csv with the clip id and the transcript\n",
    "    outFn = 'sf_pc_transcripts.zip'\n",
    "    pd.DataFrame.from_dict(transcripts, orient='index', columns=['transcript']).to_csv(outFn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98018ea6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-80de175168dfbdb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The transcripts are in the file `sf_pc_transcripts.zip`. Load them in to a `pandas` dataframe called `meetings`. (Note that you don't have to unzip first.)\n",
    "\n",
    "*Important:* Please use a *relative* filepath so that I can grade without editing your code. For example:\n",
    "- `pd.read_csv('a_file_name.csv`)` will work on any computer where the input file is in the same directory\n",
    "- `pd.read_csv('/Users/adammb/Documents/homework/a_file_name.csv')` will only work on my computer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf95984",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3933a084b5a9d8c4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "meetings = 999\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "import pandas as pd\n",
    "fn = 'sf_pc_transcripts.zip'\n",
    "meetings = pd.read_csv(fn)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e7752",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-903bdced1be3b890",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# autograder tests - do not edit\n",
    "print(len(meetings), type(meetings))\n",
    "assert len(meetings) == 857\n",
    "assert isinstance(meetings, list) or isinstance(meetings, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199d3444",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f3563fde2bd46be0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's clean up each of these transcripts. \n",
    "\n",
    "First, create a function that removes from a string:\n",
    "- excess whitespace, punctuation, and stop words\n",
    "- words of two letters or less\n",
    "- the standard nltk stopwords\n",
    "- the following words that are common in pretty much every transcript (see the suggested list in the cell below)\n",
    "\n",
    "Your function should take a string and return a cleaned-up string in the form of a list of words.\n",
    "\n",
    "*Hint*: You'll first want to use regex to remove the excess whitespace and punctuation (and then whitespace again). Then create a list of words using `split()` or `word_tokenize()` and remove the stopwords. \n",
    "\n",
    "(Don't apply the function to your transcripts yet â€” that's the next question.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b291b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be7424cd658503f9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# a list of words for you to add to the standard nltk stopwords\n",
    "extra_stopwords = ['san', 'francisco', 'board', 'supervisors', 'supervisor', 'thank', 'people', 'clerk','planning',\n",
    "           'commissioners','commissioner','commission', 'still','day','maybe','something',\n",
    "           'see','continue','okay','right','motion','thats','really','make','kind','could',\n",
    "           'city','want','president','would','like','public','please','aye', 'good','afternoon','second','may',\n",
    "          'item','next','also','think','year','time','speaker','know','one','many','much','get',\n",
    "          'great','two', 'one', 'years', 'actually', 'going', 'staff', 'hearing',\n",
    "        'three', 'inaudible', 'actual', 'yeah', 'look', 'said', 'work', 'theres', 'sure',]\n",
    "\n",
    "def clean_string(text):\n",
    "    # your code here\n",
    "    # it turns text (a string) into a cleaned list of words\n",
    "    return cleaned_list_of_words\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "swords = [re.sub(r\"[^A-z\\s]\", \"\", sword) for sword in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def clean_string(text):\n",
    "    # remove whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    # remove punctuation\n",
    "    text = re.sub(r\"[^A-z\\s]\", \"\", text)\n",
    "\n",
    "    # remove whitespace again\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    \n",
    "    cleaned_list_of_words = [word for word in word_tokenize(text.lower()) if word not in swords+extra_stopwords and len(word)>2]\n",
    "\n",
    "    \n",
    "    return cleaned_list_of_words\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9bb2b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9441894209ca2a21",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# autograder tests - do not edit\n",
    "newstr = clean_string('A    very dirty 934\\t999 string of what the San Francisco planning Commission discusses us IS like  this')\n",
    "\n",
    "print(newstr)\n",
    "assert newstr == ['dirty', 'string', 'discusses']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb8583",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8859a8c255451db1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, use your function to clean up the transcripts. Create a new column in your `transcripts` dataframe called `cleaned`. This should be a *list* of words.\n",
    "\n",
    "*Hint*: the `apply` method is the simplest way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fcf094",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ec35dcae4c0ac7e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "meetings['cleaned'] = meetings.transcript.apply(clean_string)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd53aa",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1b3b64b66a3db4ae",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# autograder tests - do not edit\n",
    "print(meetings.loc[0,'cleaned'][:50])\n",
    "print(len(meetings.loc[10,'cleaned']))\n",
    "\n",
    "assert 'cleaned' in meetings.columns\n",
    "assert meetings.loc[0,'cleaned'][:4] == ['county', 'thursday', 'welcome', 'thursday']\n",
    "assert len(meetings.loc[10,'cleaned'])==18441\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b70cb16",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-563a4b8b870f90b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Estimate an LDA topic model on transcripts. For the hyperparameters, I'd suggest `num_topics=15`, `alpha=1` and `eta=0.05` given that most meetings are likely to discuss many different topics. The challenge problem asks you to go deeper and experiment with different values, but feel free to do so here if you are inclined.\n",
    "\n",
    "Visualize your topic model using `pyLDAvis`. Remember, moving the relevance slider to the left will emphasize words that are unique to that topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c5fd7",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1a541320d85c3992",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "\n",
    "import gensim\n",
    "dictionary = gensim.corpora.Dictionary(meetings.cleaned.values)\n",
    "corpus = [dictionary.doc2bow(wl) for wl in meetings.cleaned.values]\n",
    "model = gensim.models.LdaMulticore(corpus, id2word=dictionary, num_topics=15, alpha = 1, eta=0.05)\n",
    "\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models   # note that in previous versions this was called pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(model, corpus, dictionary)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeea1e94",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a9edbdcf6653f050",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How do you interpret your topic model results, and how would you describe some of the topics? \n",
    "\n",
    "Explain in few sentences or bullet points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9749c545",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-da9d4db98f4b2f99",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4344205",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b29454a6f860b751",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, it's time for some sentiment analysis!\n",
    "\n",
    "Analyzing the sentiment of an entire transcript is probably not that helpful, because the positive and negative will balance each other out. So let's do this at the sentence level.\n",
    "\n",
    "Let's go back to our original dataframe (no need to worry about the cleaned version - sentiment analysis should be robust to the inclusion of punctuation and stop words). For each transcript (a string), we'll need to split (tokenize) it into sentences, creating a list. Then, we need to \"flatten\" (merge) these lists into a single list.\n",
    "\n",
    "Create a single list, where each element is a sentence. Call it `sentences`. It should look like this:\n",
    "\n",
    "`['Sentence 1', 'Sentence 2', 'Sentence 3']`\n",
    "\n",
    "\n",
    "*Hints*:\n",
    "- apply the `sent_tokenize()` function to the `transcript` column in your original dataframe. You import `sent_tokenize()` in the same way as `word_tokenize()`\n",
    "- you can flatten a list of lists using `itertools.chain`. For example, `import itertools` and then try `list(itertools.chain(*[[1,2],[3,4]]))`. Or Google \"flatten Python list\" for other options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401df746-7afc-4ca1-9660-626f54ba612e",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3e3b2f13feb274c8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your answer here\n",
    "sentences = 999 # replace with your code\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "import itertools\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# for each transcript, tokenize the sentences\n",
    "meetings['sentences'] = meetings.transcript.apply(sent_tokenize)\n",
    "# flatten the list\n",
    "sentences = list(itertools.chain(*meetings['sentences'].values))\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63443498-bbee-4932-a316-16acac22107b",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-22f46594b7611953",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# autograder tests - do not edit\n",
    "print(len(sentences))\n",
    "print(sentences[:2])\n",
    "assert len(sentences) == 1546925\n",
    "assert sentences[1] == '''Okay good afternoon and welcome to the san francisco planning\\n\\ncommission hearing for thursday\\n\\nMAY 22nd, 2025 when we reach the item you're interested in speaking to we ask that you\\n\\nline up on the screen side of the room or to your right.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc62647-6c5e-436b-981f-c149320ebba8",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81779abbf66a70d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Write a function, `get_sentiment()`, that calculates the sentiment score (polarity) for each comment in your (cleaned) list. \n",
    "The function should take a sentence and return a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb730134",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d7eb94c4d704bbeb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_sentiment(sentence):\n",
    "    # your code here\n",
    "    return polarity\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "from textblob import TextBlob\n",
    "def get_sentiment(sentence):\n",
    "    polarity = TextBlob(sentence).sentiment.polarity\n",
    "    return polarity\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9b6db",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0cdb129789f54d89",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# autograder tests - do not edit\n",
    "print(get_sentiment('I hate the idea of higher densities'))\n",
    "assert get_sentiment('I hate the idea of higher densities')==-0.275"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2e106f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-841198633476bfca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, apply the sentiment score to every string in your list (`sentences`). Create a new list of polarities, and assign that to a new list `sentiment_scores`.\n",
    "\n",
    "*Hint*: a list comprehension might be in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1b763d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c19976e020fc0d16",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sentiment_scores = []  # your list here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "sentiment_scores = [get_sentiment(sentence) for sentence in sentences]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45d00e1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-73ef071c85534bd3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# autograder tests - do not edit\n",
    "import numpy as np\n",
    "print(np.round(sentiment_scores[9], 2))\n",
    "assert np.round(sentiment_scores[9], 2)==0\n",
    "assert len(sentiment_scores) == len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cb41b0-50d3-4c83-a4ba-76b29cd16b96",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8f662277a6536325",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "What is the mean sentiment score? Assign it to a variable, `mean_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6cdfd9-96ef-45d8-8ae8-1b26049a06f0",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9f809ada2ba84164",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "mean_score = 999 # replace with your code\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "import numpy as np\n",
    "mean_score = np.mean(sentiment_scores)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653affc-b50f-4a39-a5d4-a435b51bc0d1",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-01eec2f5769e7057",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# autograder tests - do not edit\n",
    "print(mean_score)\n",
    "import numpy as np\n",
    "assert np.round(mean_score,3) == 0.081"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a177c",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8eeef82c46246f7d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot a histogram of your scores. Make sure to add axis labels where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd83ec39",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e393336873100a86",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "import seaborn as sns\n",
    "ax = sns.histplot(sentiment_scores)\n",
    "ax.set_xlabel('Sentiment score')\n",
    "ax.set_ylabel('Number of sentences')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff5c25-ffd6-4619-922c-deda630d329b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0ddb04c5b136696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Look at a couple of the highest and lowest-ranked sentences. What appears to be driving their sentiment? Comment in a couple of bullet points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca135b93-270c-4ceb-aadf-e1aa20a02820",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-77481ba298e53c13",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# your scratch code here\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "# I made a dataframe for easy sorting\n",
    "df = pd.DataFrame([sentences, sentiment_scores], index=['sentence','sentiment']).T\n",
    "df.sort_values(by='sentiment', inplace=True)\n",
    "\n",
    "# Now print the first few and last few rows\n",
    "print(df.sentence.head().values)\n",
    "print(df.sentence.tail().values)\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1b06f4-fc62-4478-b8f7-87e2d8c03604",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-fb5dac8bc4cedcfc",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045ed7be",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88d3e0046752901b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Challenge Problem\n",
    "Remember, you need to do at least two of these challenge problems this quarter.\n",
    "\n",
    "This challenge problem is open ended for you to take in a direction that you are most interested in. Here are some suggestions (do 1 of these in depth, 2 more quickly, or something analagous of your choice).\n",
    "\n",
    "* For the topic modeling, experiment with `num_topics`, `alpha` and `eta` to get a meaningful set of topics. You might want to clean the data further as well, e.g. through lemmatizing and dropping other words\n",
    "* How has the coverage of topics changed over time? (Note that the transcripts are in date order; the date of the meeting is also in the text.)\n",
    "* Analyze and plot the sentiment scores for sentences that mention different keywords. Perhaps you want to include the sentence before and after too. Do you see a difference for those that mention \"density,\" \"zoning\", \"housing,\" \"parking,\" \"parks,\" etc.?\n",
    "* Has sentiment changed over time?\n",
    "* Other ideas?\n",
    "\n",
    "In all cases, write some brief interpretation in a markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d87fae-e164-4f4b-ac88-5c42b7ad7e01",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5f85cc2c17bf79ac",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
