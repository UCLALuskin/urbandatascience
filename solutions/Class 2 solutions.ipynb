{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fce03f",
   "metadata": {},
   "source": [
    "## Module 2 Class activities\n",
    "This notebook is a starting point for the exercises and activities that we'll do in class.\n",
    "\n",
    "Before you attempt any of these activities, make sure to watch the video lectures for this module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092609b",
   "metadata": {},
   "source": [
    "### Scraping permit data\n",
    "Here's the code that we saw in the video lecture that queries the City of Seattle permit website, gets a dataframe of permits (including the URL), and then digs down further into that permit-specific URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89222500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the permit data from the API\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://data.seattle.gov/resource/ht3q-kdvx.json' # copied and pasted from the webpage\n",
    "r = requests.get(url)\n",
    "df = pd.DataFrame(json.loads(r.text))\n",
    "\n",
    "df = df.head(20) # get the first 5 rows, so we don't overload the city's website.\n",
    "\n",
    "# get an example link\n",
    "permiturl = df.loc[8,'link']['url']\n",
    "print(permiturl)\n",
    "\n",
    "# request that page and get the soup object\n",
    "r = requests.get(permiturl)\n",
    "soup = BeautifulSoup(r.text)\n",
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59022053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then we wrote this code to extract the project description \n",
    "links = soup.find_all('td')\n",
    "for link in links:\n",
    "    if 'Project Description' in link.text: \n",
    "        sublinks = link.find_all('td')\n",
    "        description = sublinks[1].text\n",
    "        # once we find a description, we exit\n",
    "        break\n",
    "    \n",
    "print(description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d730d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> If you look at the example, there may be a section giving information on the number of curb cuts. Extract that to a variable and print it.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hints\n",
    "# Not all of the records have the curb cut field. You can see an example here:\n",
    "# https://services.seattle.gov/portal/customize/LinkToRecord.aspx?altId=3001776-LU\n",
    "\n",
    "    # If you look at that webpage, you'll see that the text \"Number of Curb Cuts for This Permit: \"\n",
    "# is within \"span\" tags\n",
    "curbcuttext = soup.find(\"span\", string=\"Number of Curb Cuts for This Permit: \")\n",
    "\n",
    "# So to get the number of curb cuts (which is the next piece of text), \n",
    "# you can ask for the NEXT tag using find_next()\n",
    "#n_curbcuts = curbcuttext.find_next()\n",
    "\n",
    "# you'll need to add an if statement to deal with the case when this text does not exist\n",
    "    \n",
    "# your code here\n",
    "# all I did was extract the text, and add the if statement\n",
    "if curbcuttext is None:\n",
    "    n_curbcuts = np.nan\n",
    "else:\n",
    "    n_curbcuts = int(curbcuttext.find_next().text)\n",
    "print(n_curbcuts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041fefc9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Now turn that into a function that you can apply to each row of your dataframe. Add a new column, <strong>n_curbcuts</strong>, to your dataframe.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af29ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# I just copied and pasted the code above\n",
    "# and indented it into a function\n",
    "def get_curbcuts(urldict):\n",
    "    permiturl = urldict['url']\n",
    "    \n",
    "    r = requests.get(permiturl)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    \n",
    "    curbcuttext = soup.find(\"span\", string=\"Number of Curb Cuts for This Permit: \")\n",
    "    \n",
    "    if curbcuttext is None:\n",
    "        n_curbcuts = np.nan\n",
    "    else:\n",
    "        n_curbcuts = int(curbcuttext.find_next().text)\n",
    "\n",
    "    print(n_curbcuts)\n",
    "    return(n_curbcuts)\n",
    "    \n",
    "\n",
    "get_curbcuts(df.loc[8,'link'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774e339",
   "metadata": {},
   "source": [
    "### Fixing errors\n",
    "We'll do more scraping in just a moment. But first, let's do some examples of how to interpret an error message, and fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb7c600",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Each of the cells below will generate an error. Look at the error message and see if you can figure out how to fix it. (Don't Google it until you try to figure it out based on the error message.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dad38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the housingunitsremoved and housingunitsadded give useful information\n",
    "# let's create a new column with netunits\n",
    "df['netunits'] = df.housingunitsadded - df.housingunitsremoved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd4c216-6303-4b8c-9574-c6c5a54ab4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: we need to convert them to a float first\n",
    "df['netunits'] = df.housingunitsadded.astype(float) - df.housingunitsremoved.astype(float)\n",
    "df['netunits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the address of the first row\n",
    "print('Address of first row is {}. Permit type is {}'.format(df.iloc[0].originaladdress1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070f1853-2caa-4a7b-910a-b72c908a723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: We had two placeholders {} but only one variable to insert into them\n",
    "# We could delete one of the {} or add a second argument to the format()\n",
    "print('Address of first row is {}. Permit type is'.format(df.iloc[0].originaladdress1))\n",
    "print('Address of first row is {}. Permit type is {}'.format(df.iloc[0].originaladdress1, df.iloc[0].permitclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6dfde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the number of housing units to integers\n",
    "# and then summarize\n",
    "\n",
    "df['unitsadded_numeric'] = df.housingunitsadded.astype(int)\n",
    "df.unitsadded_numeric.describe("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be5fc8a-deb7-455b-bf44-17e03cfce088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: the first problem was our missing parenthesis\n",
    "\n",
    "df['unitsadded_numeric'] = df.housingunitsadded.astype(int)\n",
    "df.unitsadded_numeric.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19f95d-fdfd-40c0-896b-9aa15a054f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our second problem was the data type. An integer type cannot hold NaN\n",
    "# so we do float\n",
    "df['unitsadded_numeric'] = df.housingunitsadded.astype(float)\n",
    "df.unitsadded_numeric.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e25862",
   "metadata": {},
   "source": [
    "### Scraping craigslist\n",
    "\n",
    "In the lecture, we saw how to scrape the main page (the list of posts).\n",
    "\n",
    "What if you want to get more information about (say) a particular apartment?\n",
    "\n",
    "Go to the [craigslist housing page](https://losangeles.craigslist.org/search/apa#search=1~gallery~0~0) and copy the link for one of the listings. It should look something like this:\n",
    "https://losangeles.craigslist.org/lgb/apa/d/long-beach-home-for-rent/7597309102.html\n",
    "\n",
    "(It's fine to copy and paste the URL for now. A second step would be to loop over the URLs from the dataframe of postings that we created in the video lecture, but in class, we'll just focus on one example.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3f6d0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> For this URL, use requests to get the content of the post. (No need to create a soup object yet.)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8705a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# your code here\n",
    "# put the output of the request in a variable called r\n",
    "# so you can access the content like this\n",
    "url = 'https://losangeles.craigslist.org/ant/apa/d/valencia-charming-br-ba-with-1000-off/7838334390.html'\n",
    "r = requests.get(url)\n",
    "print(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e3794",
   "metadata": {},
   "source": [
    "Now let's extract more information from the page. We have a couple of strategies here. First, we could skip trying to parse the page with `BeautifulSoup`, and just see if particular bits of text are present.\n",
    "\n",
    "For example, what transportation modes does the post emphasize? Do they mention Section 8 vouchers? Some of this might be exploratoryâ€”we can see what type of language is included, and then parse in a more structured way (e.g. distinguishing between \"No Section 8\" and \"Section 8 welcome\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5bdf82",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Write a function that will return True if Section 8 is mentioned, otherwise False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb944b5",
   "metadata": {},
   "source": [
    "*Hint*: the `in` operator is a simple way to do this. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'plan' in 'urban planning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f97024",
   "metadata": {},
   "outputs": [],
   "source": [
    "'plan' in 'Urban Planning' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806158ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to return Section 8 information\n",
    "\n",
    "# we can use the same approach to see \n",
    "# if a string is in the text that we retrieved via requests\n",
    "# note the use of lower() to avoid case sensitivity\n",
    "'section 8' in r.text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc58c4d-90fa-41f8-99c4-f26eb9c41198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but this will return True if the text is in the string\n",
    "'los angeles' in r.text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d7636d-e751-4ad1-b2a9-64a98b681a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so let's put this in a function\n",
    "\n",
    "def sect8(url):\n",
    "   r = requests.get(url)\n",
    "   return 'section 8' in r.text.lower()\n",
    "\n",
    "# test it\n",
    "sect8(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707bacd1",
   "metadata": {},
   "source": [
    "Most of the post is free-form text. So there's not going to be much value added by `BeautifulSoup`.\n",
    "\n",
    "The exceptions are (i) parking, and (ii) the geographic coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30412ae2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Write a function that will return True if the apartment has no parking, and also returns the lat/lon of the apartment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a497c9",
   "metadata": {},
   "source": [
    "*Hint*: First, create a `soup` object. Then, look and see what tag and class encloses this information. Then, you can experiment with `find` and `find_all` with this tag and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7baf7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# replace with your own url\n",
    "url = 'https://losangeles.craigslist.org/ant/apa/d/valencia-charming-br-ba-with-1000-off/7838334390.html'\n",
    "\n",
    "# get a soup object\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad88afb-b14c-4677-bf85-7d4c09a8f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in my example, the webpage said \"carport\" in the right hand panel\n",
    "# I did a CTRL-F, and this was the result\n",
    "\n",
    "#<div class=\"attrgroup\">\n",
    "#       <div class=\"attr rent_period\">\n",
    "#        <span class=\"labl\">\n",
    "# ...\n",
    "#       <div class=\"attr\">\n",
    "#        <span class=\"valu\">\n",
    "#         <a href=\"//losangeles.craigslist.org/search/apa?parking=1\">\n",
    "#          carport\n",
    "#         </a>\n",
    "#        </span>\n",
    "#       </div>\n",
    "\n",
    "\n",
    "# so it looks like we want the class valu\n",
    "links = soup.find_all(class_='valu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58cf0f-9665-4f5e-86d7-3ad3d703d71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I see that the result was a list, with the carport in a href which includes \"parking\"\n",
    "links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ae82d2-b17d-409e-8d97-a036afc582fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so I can loop over those links, and find the one that includes \"parking\" in href\n",
    "for link in links:\n",
    "    if 'parking' in link.find('a')['href']:\n",
    "        print(link.text)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "\n",
    "# A simpler way would be to search for \"no parking\" in r.text! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7468ef-f585-4efc-ba0a-f55c76b2e90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what about lat and lon?\n",
    "# I found this in <div id=\"map\" class=\"viewposting\"\n",
    "links = soup.find_all('div', class_='viewposting')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16685672-f89d-4580-b293-632adb9a2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's in a list of length 1\n",
    "# we could get this via link = links[0]\n",
    "# or use find (which gets the first instance) rather than find_all\n",
    "link = soup.find('div', class_='viewposting')\n",
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816725ee-0e72-4cab-a192-f5e152422316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions like a dictionary object!\n",
    "lat = link['data-latitude']\n",
    "lon = link['data-longitude']\n",
    "lat, lon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731adec-f656-4816-9763-81366c3c0301",
   "metadata": {},
   "source": [
    "Now you've written this code, a next step would be to package it in a function that you can apply to all the URLs in your dataframe of posts (like the one we created in the video lecture). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bec66b-614f-4e05-81f6-af142208fbbb",
   "metadata": {},
   "source": [
    "## Large language models [Optional]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007f461-0925-4a53-b029-6a985475bd9c",
   "metadata": {},
   "source": [
    "Large language models (LLMs) such as ChatGPT can also be accessed via an API.\n",
    "\n",
    "The APIs are changing very rapidly, as are the pricing structures. For now, some LLMs are offered for free, for limited use. One such model with a free tier is Gemini, by Google.\n",
    "\n",
    "If you'd like to experiment with Gemini, you need to [get an API key here](https://aistudio.google.com/u/1/apikey). No credit card is necessary, but your UCLA Google account won't work - you need a personal Google account.\n",
    "\n",
    "[The documentation and some examples are here](https://github.com/googleapis/python-genai).\n",
    "\n",
    "The other challenge with Gemini is that its Python library has several incompatibilities, including with some of the ones we use elsewhere in the course. So you will need to create a new environment in Anaconda, using the same setup process (importing an environment) as you did at the [start of the course](https://urbandatascience.its.ucla.edu/getting-started/).\n",
    "\n",
    "Specifically:\n",
    "- In Anaconda, go to the Environments tab\n",
    "- Click on Import\n",
    "- Choose `google-genai-env.yml` under Local Drive, and call the environment `genai`\n",
    "- Manually add the `google-genai` package (choose \"Not Installed\", type `google-genai` into the search bar, select the checkbox, and then click Apply at the bottom of the screen)\n",
    "- Anaconda will take some time, before you can click Apply again to install\n",
    "- Close this notebook and open it again after you switch to your new `genai` environment\n",
    "\n",
    "All set up? Let's look at a simple example---passing a query to the chat interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e1622-dbb2-4562-b05b-0aac7a5ab7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai  # if this doesn't load, you probably have the wrong environment\n",
    "gemini_api_key = 'XXXX' #'XXXXX' # fill in your key here\n",
    "\n",
    "c = genai.Client(api_key=gemini_api_key)\n",
    "chat = c.chats.create(model='gemini-2.0-flash-001')\n",
    "response = chat.send_message('What do urban planners need to learn about gen AI?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7016bcc3-49ad-43c4-bf15-b8c00b7bea45",
   "metadata": {},
   "source": [
    "How might this be used in web scraping? \n",
    "\n",
    "Well, perhaps you can ask it to parse the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e542acd7-0dc0-486e-93fd-8e0acca0d27e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Pass the text of the craiglist post to Gemini, and ask it to return the number of parking spaces (if any). Add the result to your dataframe. NOTE: This exercise is optional (you might not want to create a Google account to get an API key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730e3ed3-eb4c-4705-b01b-e30b602bb688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "c = genai.Client(api_key=gemini_api_key)\n",
    "chat = c.chats.create(model='gemini-2.0-flash-001')\n",
    "url = 'https://losangeles.craigslist.org/ant/apa/d/valencia-charming-br-ba-with-1000-off/7838334390.html'\n",
    "\n",
    "response = chat.send_message('Does this Craiglist posting have parking? If so, which type? URL:'+url)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26491fc8-5577-4dbd-9d2a-fa2351bfe10f",
   "metadata": {},
   "source": [
    "We won't do it here, but [you can also experiment with the MetaAI API](https://github.com/Strvm/meta-ai-api). The advantage: it doesn't need an API key, but the capabilities are a bit more limited. You can also find the Meta installed in the `genai` environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2818a26",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>What you should have learned</h3>\n",
    "<ul>\n",
    "  <li>Gain confidence in experimenting with code - exploring different objects, writing functions, and so on</li>\n",
    "  <li>Learn how to extract information from a scraped webpage - how to do the detective work.</li>\n",
    "  <li>Gain confidence in debugging errors.</li>\n",
    "  <li>Learn how to integrate Large Language Models into Python</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
