{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fce03f",
   "metadata": {},
   "source": [
    "## Module 6 Class activities\n",
    "This notebook is a starting point for the exercises and activities that we'll do in class.\n",
    "\n",
    "Before you attempt any of these activities, make sure to watch the video lectures for this module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c45307",
   "metadata": {},
   "source": [
    "### Using git\n",
    "Let's start with some practice using git. \n",
    "\n",
    "I set up a class repository. You should all have full access.\n",
    "\n",
    "Enter GitHub Desktop, go to File > Clone Repository. Then clone `UCLALuskinDataScience/git-practice` to somewhere on your computer.\n",
    "\n",
    "Try adding a new file first. Create a text file on your computer and save it in the `git-practice` folder. Go to GitHub Desktop. You should see your new file.\n",
    "* Add a commit message\n",
    "* Click on \"commit to main\"\n",
    "* Fetch the origin (in case anyone has updated the repository in the meantime)\n",
    "* Click on \"push origin\"\n",
    "\n",
    "You should now see your file in the [cloud repository](https://github.com/UCLALuskinDataScience/git-practice).\n",
    "\n",
    "Fetch the origin again. Now try editing your neighbor's file, or the code that's in there already. Commit, and push it back to the cloud. \n",
    "\n",
    "What happens if you both try and edit it at once?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526e5f3",
   "metadata": {},
   "source": [
    "### ADUs and neighborhood-level predictors\n",
    "Let's continue with the example of ADUs from the video lecture.\n",
    "\n",
    "We'll add a broader set of predictors at the neighborhood (census tract) level, and see if that improves our predictive performance.\n",
    "\n",
    "First, let's load in the DataFrame that we saved during the lecture. If you ran the code for the lecture (Part 1 - Data preparation), you should be able to load it in as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f820e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "parcels = pd.read_pickle('../Lectures/scratch/joined_permits.pandas')\n",
    "\n",
    "# convert to a geodataframe. Same code as from video lecture\n",
    "parcels = gpd.GeoDataFrame(parcels, \n",
    "                    geometry = gpd.points_from_xy(\n",
    "                        parcels.CENTER_LON, \n",
    "                        parcels.CENTER_LAT, crs='EPSG:4326'))\n",
    "\n",
    "# check it looks OK\n",
    "parcels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017a779",
   "metadata": {},
   "source": [
    "Now let's load in our tract-level data from EnviroScreen. Again, this is the same code as we've used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a292986",
   "metadata": {},
   "outputs": [],
   "source": [
    "enviroscreen = gpd.read_file('../Lectures/data/CalEnviroScreen/CES4 Final Shapefile.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb38069",
   "metadata": {},
   "source": [
    "Now we can add the tract-level attributes to each parcel. (Important: we want to keep the dataset at the parcel level, rather than grouping by tract and getting counts as we did before.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c3cb0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Do a spatial join to add the EnviroScreen attributes for the relevant census tract to each parcel.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "gdf = gpd.sjoin(parcels, enviroscreen.to_crs('EPSG:4326'), how='left', predicate='intersects')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe419b47",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Estimate a random forests model that takes advantage of the new columns you just joined. Create a new variable, <strong>y_pred</strong> with your predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06466cf",
   "metadata": {},
   "source": [
    "*Hints:*\n",
    "- You'll first need to choose which variables you want. Focus on the numeric variables for now - don't worry about creating dummies from the string variables\n",
    "- Then split your dataset into training and testing portions\n",
    "- Then estimate your model\n",
    "- Then make your predictions on the testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cbab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "# choose which variables to use\n",
    "# this is a shortcut to get all\n",
    "# you could have specified a list manually\n",
    "xvars = enviroscreen.dtypes[(enviroscreen.dtypes=='float64') | (enviroscreen.dtypes=='int64')].index.tolist()\n",
    "\n",
    "# drop some that are identifiers, and add the ones from the parcels dataset\n",
    "xvars.remove('Tract')\n",
    "xvars.remove('ZIP')\n",
    "xvars += ['YearBuilt1', 'Units1', 'Bedrooms1',\n",
    "       'Bathrooms1', 'SQFTmain1', 'Roll_LandValue', 'Roll_ImpValue',\n",
    "       'Roll_LandBaseYear', 'Roll_ImpBaseYear', 'CENTER_LAT', 'CENTER_LON']\n",
    "yvar = 'hasADU'\n",
    "\n",
    "# create a dataframe with no NaNs\n",
    "df_to_fit = gdf[xvars+[yvar]].dropna()\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_to_fit[xvars], df_to_fit[yvar], test_size = 0.25, random_state = 1)\n",
    "\n",
    "# initialize the random forest classifer object\n",
    "rf = RandomForestClassifier(n_estimators = 50, random_state = 1)\n",
    "\n",
    "# now fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204ff5cc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Plot a confusion matrix, plus any other measures of fit that you think would be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e17e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363214c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Which variables are most important to your predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b92a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# convert to a series, and give the index labels from our X_train dataframe\n",
    "forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "# get the standard deviations to be able to plot the error bars\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "# sort the importances in descending order\n",
    "forest_importances.sort_values(inplace=True, ascending=False)\n",
    "\n",
    "# plot\n",
    "fig, ax = plt.subplots(figsize=(4,15))\n",
    "sns.barplot(x=forest_importances.values, y=forest_importances.index, ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca0086c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Think about how to explain your findings, and what you might do to investigate further? Discuss these with your neighbor.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818ebc06",
   "metadata": {},
   "source": [
    "Add some of your thoughts here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041fefc9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Use a neural net to estimate the probability of adding an ADU. How do your predictions compare to the random forests?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc1edb8",
   "metadata": {},
   "source": [
    "*Hint*: Remember to standardize your variables first. Since you don't have any binary (dummy) variables, you can standardize all of your x variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60729fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# your code here\n",
    "\n",
    "# scale the columns\n",
    "scaler = preprocessing.StandardScaler().fit(df_to_fit[xvars])\n",
    "df_scaled = pd.DataFrame(scaler.transform(df_to_fit[xvars]), \n",
    "                         columns=xvars, index=df_to_fit.index)\n",
    "\n",
    "# create a DataFrame with these scaled columns joined to the columns that we didn't scale\n",
    "df_scaled = df_scaled.join(df_to_fit[yvar])\n",
    "\n",
    "# standardize\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "      df_scaled[xvars], df_scaled[yvar], test_size = 0.25, random_state = 1)\n",
    "\n",
    "# estimate the neural net\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)\n",
    "\n",
    "# predictions\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c292bac",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Experiment with some of the hyperparameters (e.g. layer sizes). How do these affect your results?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ca354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09baf9bb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Map your predictions. You could create a census-tract level variable with the predicted probability of a parcel having an ADU. You could also map the predicted vs actual ADU numbers.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2818a26",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>What you should have learned</h3>\n",
    "<ul>\n",
    "  <li>Gain more practice with spatial joins</li>\n",
    "  <li>Understand how to estimate a random forests model.</li>\n",
    "  <li>Understand how to interpret the results of machine learning classification models.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
