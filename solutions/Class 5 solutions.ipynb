{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53fce03f",
   "metadata": {},
   "source": [
    "## Module 5 Class activities\n",
    "This notebook is a starting point for the exercises and activities that we'll do in class. We'll do an extension of the random forests classifier, looking at a continuous variable.\n",
    "\n",
    "Before you attempt any of these activities, make sure to watch the video lectures for this module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092609b",
   "metadata": {},
   "source": [
    "### Classification: NYC evictions\n",
    "We'll look at the factors that are associated with evictions in New York City. Perhaps a machine learning model can identify the types of places that are vulnerable to eviction, and target renter assistance programs more effectively?\n",
    "\n",
    "#### Loading in the data\n",
    "\n",
    "Let's start by loading in the [eviction dataset](https://data.cityofnewyork.us/City-Government/Evictions/6z8x-wfk4) via Socrata."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30faed6",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<strong>Exercise:</strong> Import the data from Socrata via the API into a pandas DataFrame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b86bd",
   "metadata": {},
   "source": [
    "*Hints*:\n",
    "- Look back at Week 1 if you need a refresher on using Socrata\n",
    "- There are about 70,000 rows in the dataset. So remember to add `?$limit=100000` to the end of the URL that you pass to `requests.get()`. Otherwise, you'll just get the first 1,000 rows. (The limit can be anything comfortably above 70000.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# your code here\n",
    "\n",
    "url = 'https://data.cityofnewyork.us/resource/6z8x-wfk4.json?$limit=100000'\n",
    "r = requests.get(url)\n",
    "df = pd.DataFrame(json.loads(r.content))\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9549f5a2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<strong>Exercise:</strong> Convert your dataframe to a GeoDataFrame, using the latitude and longitude columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e02f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here \n",
    "gdf = gpd.GeoDataFrame(df, \n",
    "            geometry = gpd.points_from_xy(\n",
    "                df.longitude, df.latitude, crs='EPSG:4326'))\n",
    "gdf.plot() # make sure it looks ok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5657ee",
   "metadata": {},
   "source": [
    "Now let's import some census data. We could use `cenpy` or the Census Bureau API. But to keep things simple so that we can focus on the spatial joins and the machine learning, I downloaded the block group-level 2019 ACS data for New York from the [Census Bureau](https://www.census.gov/geographies/mapping-files/time-series/geo/tiger-data.html). To save space, I clipped it to the 5 NYC counties.\n",
    "\n",
    "It's in your repository, and we can load it in as follows. If you aren't familiar with a GeoPackage (GPKG) format, think of it as a \"new and improved shapefile.\" [Here's a good overview.](https://towardsdatascience.com/why-you-need-to-use-geopackage-files-instead-of-shapefile-or-geojson-7cb24fe56416)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d4aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs = gpd.read_file('../classes/data/nyc_bgs.gpkg')\n",
    "bgs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c295e3d",
   "metadata": {},
   "source": [
    "Note that the variables aren't particularly carefully selected - I just threw in many of the demographic and housing variables. \n",
    "\n",
    "Nor are the variable names particularly informative, but the full names are in a file in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86627295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note it is tab-sepated, not comma separated\n",
    "# so we use the sep='\\t' argument\n",
    "\n",
    "col_names = pd.read_csv('../classes/data/BG_METADATA_2019.txt', sep='\\t', index_col='Short_Name')\n",
    "col_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ea2c7",
   "metadata": {},
   "source": [
    "So you can see the definition of the column like this. (I don't recommend renaming the `bg` column names, because the full names are so long.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59fcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names.loc['B01001e1']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da707b5",
   "metadata": {},
   "source": [
    "#### Spatial join\n",
    "Now let's do the spatial join. Again, let's follow our three step process.\n",
    "\n",
    "1. Use a spatial join to add the `GEOID` column to the evictions dataframe. *Hint:* Check your projections.\n",
    "2. Group by `GEOID` to get a count of evictions per block group. If you have a `Series`, give it a name - maybe `n_evictions`\n",
    "3. Join those counts back - a tabular join based on the index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd0b18",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Add a count of evictions per census block group to your <strong>bgs</strong> GeoDataFrame, using the 3-step process above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc447f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "# spatial join\n",
    "# match the CRS\n",
    "bgs.to_crs('EPSG:4326', inplace=True)\n",
    "\n",
    "gdf2 = gpd.sjoin(gdf, bgs, predicate='intersects')\n",
    "\n",
    "# count \n",
    "n_evictions = gdf2.groupby('GEOID').size()\n",
    "n_evictions.name = 'n_evictions'\n",
    "\n",
    "# join back\n",
    "bgs = bgs.set_index('GEOID').join(n_evictions)\n",
    "bgs.fillna({'n_evictions':0}, inplace=True)\n",
    "bgs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89523f4a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Do a quick-and-dirty map of the number of evictions. This will help identify any data holes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bgs.plot('n_evictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b5b4f",
   "metadata": {},
   "source": [
    "#### Random forests regressor\n",
    "Now we have our data set. Let's estimate a random forests model.\n",
    "\n",
    "In contrast to the examples in the lecture, we are trying to predict a continuous variable - the number of evictions. So our classifier isn't appropriate. \n",
    "\n",
    "However, there is a similar model: the [random forest regressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor). It works almost identically to the classifier. The main difference from a user perspective is assessing model performance - a confusion matrix doesn't work here.\n",
    "\n",
    "You'll need to follow the following steps:\n",
    "- choose your x variables. (Your y variable will be `n_evictions`)\n",
    "- Drop Null values if needed\n",
    "- split your dataset into training and testing portions\n",
    "- estimate (fit) the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24327cee",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Estimate a random forest regressor model to predict the number of evictions per census tract.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192468ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# your code here\n",
    "# I'm just going to throw in all of the columns\n",
    "xvars = [col for col in bgs.columns if col not in ['geometry','n_evictions']]\n",
    "yvar = 'n_evictions'\n",
    "\n",
    "# create a dataframe with no NaNs\n",
    "df_to_fit = bgs[xvars+[yvar]].dropna()\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_to_fit[xvars], df_to_fit[yvar], test_size = 0.25, random_state = 1)\n",
    "\n",
    "# initialize the random forest classifer object\n",
    "rf = RandomForestRegressor(n_estimators = 50, random_state = 1)\n",
    "\n",
    "# now fit the model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144a7569-2c26-4128-a955-785770019dc7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Examine some of your trees in the random forest. What do they tell you?</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f57cb-c9bb-447a-9f76-b4006519cd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a start, and gives a sense of the complexity of the model\n",
    "# but you might want to plot just a portion of the tree. \n",
    "\n",
    "e = rf.estimators_[0]\n",
    "\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(40,20))  \n",
    "_ = tree.plot_tree(e, feature_names = X_train.columns, fontsize=15, ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703389ef-e98d-4aeb-8f8c-74b81d868b66",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Experiment with different model hyperparameters and variables. Discuss your rationale and the results with a neighbor.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c6bb8-6505-4ec9-ba98-4b61c6ea1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc81ca4-72ff-424d-9ec0-d3bd1cc7c70b",
   "metadata": {},
   "source": [
    "The following questions relate to some of the material in Module 6. You might want to wait until watching those lectures. Then come back and complete these tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5483eb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Assess the fit of your model.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded99af",
   "metadata": {},
   "source": [
    "Remember, the confusion matrix and accuracy scores don't apply here. Some ideas for continuous variables are [here](https://stackoverflow.com/questions/50789508/random-forest-regression-how-do-i-analyse-its-performance-python-sklearn). You could also plot actual vs predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779701e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# I used mean absolute error\n",
    "print(metrics.mean_absolute_error(y_test, y_pred))\n",
    "\n",
    "# but the scatter plot is more helpful to me\n",
    "fig, ax = plt.subplots()\n",
    "sns.regplot(x=y_test, y=y_pred, ax=ax)\n",
    "ax.set_xlabel('Actual evictions')\n",
    "ax.set_ylabel('Predicted evictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5e2a1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Which variables are most important in your predictions? Plot the forest importances.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d83e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# code from https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# convert to a series, and give the index labels from our X_train dataframe\n",
    "forest_importances = pd.Series(importances, index=X_train.columns)\n",
    "\n",
    "# get the standard deviations to be able to plot the error bars\n",
    "# acutally, these are no longer supported (for now) in seaborn, so we won't plot them\n",
    "std = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n",
    "\n",
    "# sort the importances in descending order\n",
    "forest_importances.sort_values(inplace=True, ascending=False)\n",
    "\n",
    "# let's plot just the top 10\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,10))\n",
    "sns.barplot(x=forest_importances.values[:10], y=forest_importances.index[:10], ax=ax)\n",
    "ax.set_title(\"Feature importances using MDI\")\n",
    "ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "\n",
    "# let's find out what those variable are\n",
    "# it would be better to actually change the axis tick label, but this is quick and dirty\n",
    "\n",
    "for col_name in forest_importances.index.values[:10]:\n",
    "    print(col_names.loc[col_name].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2818a26",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>What you should have learned</h3>\n",
    "<ul>\n",
    "  <li>Get more practice with spatial joins and Socrata.</li>\n",
    "  <li>Learn how to estimate a random forests model for continuous data.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
