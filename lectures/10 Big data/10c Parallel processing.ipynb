{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01c4105",
   "metadata": {},
   "source": [
    "# Parallel processing\n",
    "## Lecture objectives\n",
    "\n",
    "1. Demonstrate how to make use of multiple cores on your computer, through running processes in parallel\n",
    "2. Introduce (conceptually) the MapReduce framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd8f2b5",
   "metadata": {},
   "source": [
    "## How to parallelize locally\n",
    "Even basic laptops usually come with multiple cores. Some functions, like the `scikit-learn` machine learning models, have a parameter that you can set to use more than one core (e.g. `n_jobs=-1` in `RandomForestClassifier`). \n",
    "\n",
    "Postgres and other databases can also make use of multiple cores to do CPU-intensive operations such as spatial joins. They also write intermediate results to disk so help with memory too (a SSD helps). We'll look at databases in the next lecture.\n",
    "\n",
    "In other cases, you'll need to do the parallelization yourself. Python helps through the `multiprocessing` library. The key is to set up discrete *functions* that can then be run in parallel on different subsets of the data.\n",
    "\n",
    "Let's recreate the example we used in Module 4, to calculate the distance from every census tract to the *third* closest food pantry. As a reminder, this is a common measure of accessibility. See the lecture, *Advanced spatial joins*, for a refresher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c658b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygris\n",
    "import requests\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "\n",
    "# get the census tract boundaries for LA\n",
    "tracts = pygris.tracts(state='06',county='037', year=2019)\n",
    "\n",
    "# get the food pantries\n",
    "pantryDf = pd.read_csv('../data/Food_Resources_in_California.csv')\n",
    "pantryDf = pantryDf[pantryDf.County=='Los Angeles']\n",
    "pantries = gpd.GeoDataFrame(\n",
    "    pantryDf, geometry=gpd.points_from_xy(pantryDf.Longitude, pantryDf.Latitude, \n",
    "                                          crs='EPSG:4326'))\n",
    "# project both to state plane\n",
    "pantries.to_crs('EPSG:3497', inplace=True)\n",
    "tracts.to_crs('EPSG:3497', inplace=True)\n",
    "pantries.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7478fe9",
   "metadata": {},
   "source": [
    "Let's slow things down by making 100 duplicates of the `pantries` geodataframe. \n",
    "\n",
    "Here, we use a list comprehension to create a list of 50 duplicates. Then, we use `pd.concat()` to stack (concatenate) them into a single geodataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72df12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pantries = pd.concat([pantries for i in range(100)]).reset_index()\n",
    "pantries.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03443b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function that we had before to return the distance of the closest pantry to each tract\n",
    "def get_3rd_closest_dist(geom):\n",
    "    distances = pantries.distance(geom)\n",
    "    third_closest = distances.sort_values().iloc[2]\n",
    "    return third_closest\n",
    "\n",
    "%time tracts['dist_third_closest'] = tracts.geometry.apply(get_3rd_closest_dist)\n",
    "tracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8100c4d5",
   "metadata": {},
   "source": [
    "That was pretty fast, but again, think of a situation when we are doing this for (say) all census tracts and all grocery stores in the country.\n",
    "\n",
    "Let's see if parallel processing can work. Don't worry too much the syntax - there are lots of examples online if and when you need to use this. But I want you to understand it conceptually.\n",
    "\n",
    "One complication of multiprocessing in Jupyter Lab is that you have to import the function from disk. You can't define it in a cell. So I saved `parallelization_example.py` in the repository, which contains the function above. It's the same as above, except there is a second argument, `pantries`. Basically, a multiprocessed function has to be self-contained - you have to pass all the arguments that it will need, rather than relying on them being available as global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37702de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "\n",
    "# import the function we saved to disk\n",
    "from parallelization_example import get_3rd_closest_dist\n",
    "\n",
    "# use 4 cores\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    # this splits the function up, so that pantries is always passed\n",
    "    # see https://stackoverflow.com/questions/15331726/how-does-functools-partial-do-what-it-does\n",
    "    func = partial(get_3rd_closest_dist, pantries)\n",
    "    # now we pass the geometries to our function, but split up over multiple cores\n",
    "    results = pool.map(func, tracts.geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2c635",
   "metadata": {},
   "source": [
    "Quite a nice speed up. Note that there is some overhead, so using 4 cores only means a ~67% speedup in this case.\n",
    "\n",
    "What's happening? Each row of `tracts` is being passed to a pool. As soon as one of the 4 cores is free, it accepts the next job from the pool.\n",
    "\n",
    "Multiprocessing works when *none of the jobs are dependent on each other*. It's hard to pass information between different jobs in the pool.\n",
    "\n",
    "But how do we access our results? Let's look at the `results` object that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ae917",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67095352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first few elements\n",
    "results[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e803d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2737ef",
   "metadata": {},
   "source": [
    "This is the same length as our original `tracts` dataframe. We had 749 geometries to process, and our function returned a distance from each one. So we can put the results back into our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f86c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts['dist_third_closest'] = results\n",
    "tracts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2645953b",
   "metadata": {},
   "source": [
    "Let's check - yes, this is the same result as we got without parallelization (see above)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43ca61f",
   "metadata": {},
   "source": [
    "## MapReduce\n",
    "MapReduce is a related concept to parallel processing. [See the details of the algorithm here.](https://en.wikipedia.org/wiki/MapReduce) In short, there is a *map* stage (filtering and sorting), and a *reduce* stage (reducing the input data to something smaller). Each of the mapper and reducer nodes can run in parallel.\n",
    "\n",
    "A canonical example is word counting.\n",
    "\n",
    "![mapreduce](https://upload.wikimedia.org/wikipedia/commons/0/03/WordCountFlow.JPG)\n",
    "\n",
    "Map Reduce is normally done in the cloud (there are specialist tools like Hadoop), but you could do it on a local machine in Python as well. Conceptually, think about what are the discrete, independent chunks that you can split your tasks into."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67974028",
   "metadata": {},
   "source": [
    "## Outgrowing your local machine\n",
    "Almost all of my own data science work is run locally on my office computer. Currently, I have a 2018 Mac Mini, and my previous desktop was from 2008, so it's not like you need a lot of computing power to do most tasks. You might need a large external hard drive.\n",
    "\n",
    "Sometimes it takes a few days to a week, running in the background. Sometimes I have to parallelize, or be judicious about data types. But for me, it's often more convenient than uploading to (and paying for) a cloud service.\n",
    "\n",
    "For some jobs, however, you will outgrow your laptop. We won't say much about other options in this course, but in general there are three options:\n",
    "* Beg, steal, borrow. Can you get access to a computer in the GIS lab to run over the weekend or holidays?\n",
    "* UCLA's Hoffman2 cluster. You can run pretty much any Python code as a \"batch job,\" meaning that your job will be queued until there are resources available for it to run. [See the details here.](https://www.hoffman2.idre.ucla.edu) \n",
    "* Cloud-based Python services like [DeepNote](https://deepnote.com/education) (basic plan free for educational users) or Python Anywhere. The amount of memory and compute power varies, but generally you pay for what you use.\n",
    "* More general cloud services like AWS, Google Cloud, Microsoft Azure, etc. These let you run Python and pretty much any other open-source tool, including databases.\n",
    "\n",
    "These options are changing all the time, so keep an eye out for new firms and services that are being offered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac96c47",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>Parallel processing works when you are applying an identical function to many different inputs - census tracts, PDF files, large dataframes, etc. There cannot be interdependencies.</li>\n",
    "  <li>You will normally be able to run any dataset on a standard laptop by using these tools, but the UCLA Hoffman2 cluster and cloud service providers are there if you need them.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
