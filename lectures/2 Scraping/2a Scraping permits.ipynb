{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sunrise-lease",
   "metadata": {},
   "source": [
    "# Module 2. Web scraping\n",
    "# Scraping permits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extreme-train",
   "metadata": {},
   "source": [
    "## Lecture objectives\n",
    "\n",
    "1. Demonstrate how to scrape web pages and other data where an API doesn't exist\n",
    "2. Introduce the `BeautifulSoup` library\n",
    "3. Provide more practice with `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-fantasy",
   "metadata": {},
   "source": [
    "APIs make it relatively simple to get data from the web. But sometimes, an API doesn't exist—they take effort on the part of the agency to set up and maintain.\n",
    "\n",
    "In these cases, we can still obtain data from the web. But rather than dropping it directly into a (geo)pandas `DataFrame`, we'll need to do more work to understand the structure of the webpage, and to clean and process the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-newton",
   "metadata": {},
   "source": [
    "## Example: Land use permit data\n",
    "Often, cities make their building and land use permit data available for download, and/or accessible through an API. But these are typically incomplete—they provide a subset of fields that are most relevant to most users (e.g., permit approval date and number of units), but perhaps exclude more esoteric fields. And parking, sadly, is one of the fields that is often excluded.\n",
    "\n",
    "For a [recent project](https://www.tandfonline.com/doi/full/10.1080/01944363.2021.1873824), I looked at the impacts of TOD plans in Seattle and San Francisco on development outcomes, including parking ratios. Let's walk through how I obtained the data for the Seattle analysis.\n",
    "\n",
    "The basic Seattle land use permit dataset [is available through the city's Socrata API](https://data.seattle.gov/Permitting/Land-Use-Permits/ht3q-kdvx). That's a good starting point for our work. Let's get this into a `pandas` dataframe, in the same way that we did with the Los Angeles data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "url = 'https://data.seattle.gov/resource/ht3q-kdvx.json' # copied and pasted from the webpage\n",
    "r = requests.get(url)\n",
    "df = pd.DataFrame(json.loads(r.text))\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e452b6db",
   "metadata": {},
   "source": [
    "There are lots of columns, so the output is truncated.\n",
    "\n",
    "But we can explore the contents of the dataframe in other ways. For example `.info()` gives us the column names and variable types. (Object is normally a string, or a mixed type.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d35202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electoral-pension",
   "metadata": {},
   "source": [
    "Notice that there is a `link` field. Let's take a look at the first one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlimited-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .loc operator gives us an extract from the dataframe. 0 is the index, 'link' is the column\n",
    "# So this gives us the contents of the 'link' column for the row with index 0 (the first one).\n",
    "\n",
    "print(df.loc[0, 'link'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-fisher",
   "metadata": {},
   "source": [
    "Notice that this column of the pandas dataframe is a dictionary. That's perhaps a surprise, but we know how to deal with dictionaries. \n",
    "\n",
    "For now, [let's take a look at what one of these links looks like](https://cosaccela.seattle.gov/portal/customize/LinkToRecord.aspx?altId=3001212-LU). Clearly, there is a lot more information here about the specific permit, than is provided via the API!\n",
    "\n",
    "How do we bring the information in that webpage into Python? Remember, the `requests` library is our friend in this circumstance. While we've used it to get data from an API, `requests` can retrieve pretty much anything from the web.\n",
    "\n",
    "First, let's extract the text string that gives the URL for this row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-hollywood",
   "metadata": {},
   "outputs": [],
   "source": [
    "urldict = df.loc[0,'link']\n",
    "print(urldict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1ae4a9",
   "metadata": {},
   "source": [
    "As we saw before, it's a dictionary with a key of 'url', so let's extract the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4d2d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "permiturl = urldict['url']\n",
    "print(permiturl)\n",
    "\n",
    "# or we could do this in one step: permiturl = df.loc[0,'link']['url']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f8f405",
   "metadata": {},
   "source": [
    "Now, pass that URL to `requests` in the same way that we did for the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85c10b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(permiturl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bde306",
   "metadata": {},
   "source": [
    "Let's look at what requests has returned. \n",
    "\n",
    "Remember, the `.text` attribute gives us the text of what's retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-thirty",
   "metadata": {},
   "source": [
    "### Using BeautifulSoup\n",
    "It looks like we've got the whole .html webpage. The relevant information is buried in there, but how can we get it in the sea of html code?\n",
    "\n",
    "This is where the `BeautifulSoup` library comes in ([documentation here](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)). Let's convert our text to a \"soup\" object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(r.text, features='html.parser')\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-mobility",
   "metadata": {},
   "source": [
    "This soup object has a lot of attributes and functions (type `soup.` and press tab to autocomplete). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22aa7db",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Look at what you can do with the <strong>soup</strong> object. Experiment. What functions seem most useful?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f7371",
   "metadata": {},
   "source": [
    "Let's suppose we want to get information the project description (where the parking information might be included, since there isn't a separate parking field). (In reality, the \"description\" field is in the API version, but that wasn't the case originally, and it's good practice.)\n",
    "\n",
    "Just like with the API output that we saw earlier, extracting this is a case of step-by-step detective work.\n",
    "\n",
    "If you look at the [output](https://cosaccela.seattle.gov/portal/cap/CapDetail.aspx?type=1000&fromACA=Y&agencyCode=SEATTLE&Module=DPDPermits&capID1=05HST&capID2=00000&capID3=19806) in the Develop mode in your web browser, it seems that Project Description is contained within a `<td>` tag. \n",
    "\n",
    "We'll use the `.find_all()` function to find the relevant text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds = soup.find_all('td') # returns a \"list-like\" object, i.e. we can loop through it or slice it like a list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9753f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> What is the <strong>tds</strong> object? How can you make use of it?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee8330",
   "metadata": {},
   "source": [
    "Let's have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23461d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3e80b5",
   "metadata": {},
   "source": [
    "What on earth is a `ResultSet`? The [docs](https://tedboy.github.io/bs4_doc/generated/generated/bs4.ResultSet.html) tell us that it's a list. So we can use our regular methods to look at a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035121a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the first element\n",
    "print(tds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179c97ca",
   "metadata": {},
   "source": [
    "More systematically, let's loop through to find the element that has the Project Description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbc612",
   "metadata": {},
   "outputs": [],
   "source": [
    "for td in tds:\n",
    "    if 'Project Description' in td.text: \n",
    "        # stop here and abort the loop\n",
    "        break \n",
    "        \n",
    "print (td) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-preserve",
   "metadata": {},
   "source": [
    "Now we are getting closer! It looks like the Project Description is contained in another `<td>` tag, nested one level down. So let's do the same thing again at this second-level link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-messaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "tds2 = td.find_all('td')\n",
    "print(tds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-frank",
   "metadata": {},
   "source": [
    "We've obtained a list! And the information we need is in the second element of that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-hygiene",
   "metadata": {},
   "outputs": [],
   "source": [
    "description = tds2[1]\n",
    "print(description.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-covering",
   "metadata": {},
   "source": [
    "Now, let's take everything we've done so far, and put it in a function. To help with that, let's recap.\n",
    "\n",
    "* For each row of the `DataFrame` (`df`), we have a dictionary with the url (we called that `urldict`)\n",
    "* We extracted the URL from that dictionary, and put it in the `permiturl` variable\n",
    "* We requested that URL using requests, and put the response in the `r` variable\n",
    "* We converted that response to a `soup` object, and called it `soup`\n",
    "* We found all the content within `td` tags, and put that in the `tds` variable (a `ResultSet` or list)\n",
    "* We looped over each element of `tds`, and found the one that contains \"Project Description\"\n",
    "* We found all the content within the second level of `td` tags, and put the second element in the `description` variable\n",
    "* We extracted the text from that `description` variable, using the `text` attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388a59a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Identify each of these steps in the code above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08c390b",
   "metadata": {},
   "source": [
    "That's a lot of steps! So let's write a function that allows us to apply all of these steps to each permit.\n",
    " \n",
    "The function takes a single argument: the dictionary in the `url` column of the pandas DataFrame\n",
    " \n",
    "It returns the Description text, unless that's not found, in which case it returns an empty string `''`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scheduled-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDescription(urldict):\n",
    "    permiturl = urldict['url']\n",
    "    r = requests.get(permiturl)\n",
    "    soup = BeautifulSoup(r.text, features='html.parser')\n",
    "    tds = soup.find_all('td')\n",
    "    for td in tds:\n",
    "        if 'Project Description' in td.text: \n",
    "            tds2 = td.find_all('td')\n",
    "            description = tds2[1]\n",
    "            # once we find a description, we return it and exit the function\n",
    "            return description.text \n",
    "    \n",
    "    return '' # if we don't find it, return an empty string\n",
    "\n",
    "# Now let's apply this function to the first link in our dataframe\n",
    "urldict = df.loc[0,'link']\n",
    "getDescription(urldict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-prompt",
   "metadata": {},
   "source": [
    "The advantage of a function is that we can now apply this procedure to every row of our pandas DataFrame.\n",
    "\n",
    "Let's do this for 10 rows (so we are nice and don't disrupt the City's website).\n",
    "\n",
    "The `apply` function in `pandas` applies a function to each row of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of the first 10 rows of the dataframe.\n",
    "smalldf = df.head(10).copy()  \n",
    "\n",
    "# for each row in smallDf, we pass the link column to getDescription\n",
    "# That will then appear within the function, but be called urldict (the name of the argument to the function) \n",
    "descriptions = smalldf['link'].apply(getDescription)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does the function return? It's a pandas Series (basically, a one-column DataFrame)\n",
    "print(type(descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-noise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can insert that into the dataframe as a new column\n",
    "smalldf['newdescription'] = descriptions\n",
    "# we could have done this in one step: \n",
    "# smalldf['newdescription'] = smalldf['link'].apply(getDescription) \n",
    "smalldf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-scotland",
   "metadata": {},
   "source": [
    "### Saving the file\n",
    "Let's stop here for the moment. But in the next lecture, we'll come back to this dataset. Rather than request the data again from the server, we can save it locally as a file.\n",
    "\n",
    "If you type `smalldf.to_` and tab complete, you'll see options for lots of different file formats. Typically, we'll use `to_csv()` (a plain text .csv format), or `to_pickle()` (which saves the `pandas` object). .csv is more reliable for longer-term storage and for sharing information, but is slower and loses some of the properties of the data frame.\n",
    "\n",
    "So let's save the `pandas` object to the `scratch` folder in your repository. The `..` means \"go back one level,\" i.e. to the enclosing folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "smalldf.to_pickle('../scratch/Seattle_permits.pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-trout",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Let's generalize.</strong> What did we do here?\n",
    "    \n",
    "1. We obtained the URL for each page to scrape. (Here, it was given to us in the city's data file, but sometimes we'll have to reverse-engineer the composition of the URL.)\n",
    "2. We examined a sample page, and identified the html tags that enclose the data we wanted to extract.\n",
    "3. We wrote a function that pulled out the data for a specific page.\n",
    "4. We applied that function to each URL / page. Since our URLs were in a pandas DataFrame, we could use the pandas <strong>apply</strong> method.\n",
    "    \n",
    "Every scraping project will pose different challenges, but normally it will involve each of these four steps.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
