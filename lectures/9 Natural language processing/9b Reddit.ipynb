{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7853aaf0",
   "metadata": {},
   "source": [
    "# Working with Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e9f6b",
   "metadata": {},
   "source": [
    "## Lecture objectives\n",
    "1. Demonstrate how to scrape Reddit data using their API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505057a",
   "metadata": {},
   "source": [
    "We reviewed topic modeling in the previous lecture. Here and in the next lecture, we'll focus on another common Natural Language Processing tool: sentiment analysis. In short, sentiment analysis tries to understand whether a snippet of text (e.g. a tweet, a review, or a sentence from an article) is positive, negative, or neutral.\n",
    "\n",
    "We'll apply sentiment analysis to some Reddit data on public transportation, using the [PRAW](https://praw.readthedocs.io/en/stable/) library.\n",
    "\n",
    "If you want to access the Reddit data yourself, you'll need to:\n",
    "\n",
    "(1) sign up for a Reddit account (free)\n",
    "\n",
    "(2) create a client id and client secret. It's also free, and takes about 5 minutes. [Follow the second part of these instructions.](https://cs205uiuc.github.io/guidebook/python/reddit-api.html) You can get to the apps tab here: https://www.reddit.com/prefs/apps.\n",
    "\n",
    "In earlier versions of this course, I used Twitter. However, academic access to the Twitter API is no longer free. For a thorough treatment of obtaining, analyzing, and interpreting Twitter data, check out [*Twitter as Data*](https://www.cambridge.org/core/elements/twitter-as-data/27B3DE20C22E12E162BFB173C5EB2592) by Prof. Zachary Steinert-Threlkeld here in the Luskin School of Public Affairs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d434b4",
   "metadata": {},
   "source": [
    "## Using the Reddit API\n",
    "The `praw` library provides easy access to Reddit. You can enter your credentials here, or just follow along for the time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c440448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "# enter your own client_id and client_secret\n",
    "client_id = 'YOUR_ID_HERE'\n",
    "client_secret = 'YOUR_SECRET_HERE'\n",
    "# this identifies you, but can be any string\n",
    "user_agent = 'scraper by u/adammb_ucla'\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801173a2",
   "metadata": {},
   "source": [
    "Now we have our `reddit` object that has several methods.\n",
    "\n",
    "For example, we can get new posts, and loop over them. Here's we'll get the latest 10 from the Urban Planning subreddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83c8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission in reddit.subreddit('urbanplanning').new(limit=10):\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f29028c-0c11-4d83-9ca2-7a45d2ec7fe9",
   "metadata": {},
   "source": [
    "The `submission` object provides access to the more detailed post information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2dccf1-e5a0-4d8f-be30-037dd5719f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992ff103",
   "metadata": {},
   "source": [
    "Let's look at the comments within the last submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8dcfa3-6a73-46d1-b767-6d9f03e87be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.num_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d144d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in submission.comments:\n",
    "    print(c.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd379c8",
   "metadata": {},
   "source": [
    "Each comment `c` also has further attributes. We used `body` to get the text of the comment, but there are also timestamps, author details, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e3f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b21adcd",
   "metadata": {},
   "source": [
    "Let's get the submission and all the comments from the latest 100 posts from three transit subreddits: LA Metro, BART, and NYC rail.\n",
    "\n",
    "We'll define a function that takes the subreddit name, and returns all of these comments in a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82a7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit(subreddit_name):\n",
    "    r_list = []\n",
    "    for submission in reddit.subreddit(subreddit_name).new(limit=50):\n",
    "        r_list.append(submission.title)\n",
    "        r_list += [c.body for c in submission.comments] \n",
    "\n",
    "    print('Retrieved {} comments for {}'.format(len(r_list), subreddit_name))\n",
    "    return r_list\n",
    "\n",
    "la_metro = get_reddit('LAMetro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c4e3ac",
   "metadata": {},
   "source": [
    "Let's do the same for the other two agencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c883f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart = get_reddit('Bart')\n",
    "nyc_rail = get_reddit('Nycrail')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57533d68",
   "metadata": {},
   "source": [
    "Now let's save these comments to a file. We'll use a pickle, which as we've seen in earlier modules, can save most Python objects in their original format. (We could also have looped over the list of comments and saved them as text.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f053c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/reddit/la_metro.pickle', 'wb') as f:\n",
    "    pickle.dump(la_metro, f)\n",
    "with open('../data/reddit/bart.pickle', 'wb') as f:\n",
    "    pickle.dump(bart, f)\n",
    "with open('../data/reddit/nyc_rail.pickle', 'wb') as f:\n",
    "    pickle.dump(nyc_rail, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c9784",
   "metadata": {},
   "source": [
    "We'll pick up these data in the next lecture and see how to analyze the sentiment of the Reddit posts.\n",
    "\n",
    "But we have only scratched the surface of the PRAW library. Explore the documentation for examples of how to filter your results (e.g. you could search for all posts within a subreddit that mention \"rent\" or \"eviction\"), access the number of upvotes, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce3eca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>Reddit has a powerful API that is relatively easy to use.</li>\n",
    "  <li>Reddit is not representative. Whether that matters depends on your particular project and use case.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
