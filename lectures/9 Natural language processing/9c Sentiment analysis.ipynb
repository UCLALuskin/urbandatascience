{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7853aaf0",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e9f6b",
   "metadata": {},
   "source": [
    "## Lecture objectives\n",
    "1. Demonstrate how to implement and interpret sentiment analysis\n",
    "2. Discuss the use of polarity and subjectivity scores\n",
    "3. Explore how to create figures with multiple subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3505057a",
   "metadata": {},
   "source": [
    "In its simplest form, sentiment analysis works through applying a corpus of words and phrases that indicate sentiment. [SentiWordNet](https://github.com/aesuli/SentiWordNet) is a commonly used corpusâ€”[look at the list here](https://raw.githubusercontent.com/aesuli/SentiWordNet/master/data/SentiWordNet_3.0.0.txt). For example, \"worst,\" \"terrible,\" and \"apprehensive\" have negative scores, while \"feel_like_a_million_dollars\" has a positive score. Some words have both positive and negative scores. Some algorithms consider the part of speech in which the word occurs (e.g. is it an adjective or noun).\n",
    "\n",
    "In this lecture, we'll use Reddit as our source of data. However, sentiment analysis can be applied to any other opinionated source of data, such as Twitter (as in [Schweitzer 2014](https://www.tandfonline.com/doi/abs/10.1080/01944363.2014.980439)) or Yelp (as in [Jiang and Mondschein 2021](https://link.springer.com/article/10.1007/s42421-021-00036-1)).\n",
    "\n",
    "Let's start through loading in the Reddit comments that we saved in the previous lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f053c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../data/reddit/la_metro.pickle', 'rb') as f:\n",
    "    la_metro = pickle.load(f)\n",
    "with open('../data/reddit/bart.pickle', 'rb') as f:\n",
    "    bart = pickle.load(f)\n",
    "with open('../data/reddit/nyc_rail.pickle', 'rb') as f:\n",
    "    nyc_rail = pickle.load(f)\n",
    "la_metro[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b7b811",
   "metadata": {},
   "source": [
    "Let's turn to sentiment analysis. `textblob` uses the corpora (basically, a body of text) from the `nltk` library. We already downloaded a couple of corpora such as stop words, but we need two more. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4302379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe569d1",
   "metadata": {},
   "source": [
    "Let's try some examples. Note from the [documentation](https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis): \n",
    "\n",
    "* The sentiment property returns a named tuple of the form `Sentiment(polarity, subjectivity)`. \n",
    "* The polarity score is a float within the range [-1.0, 1.0].\n",
    "* The subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc349440",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = TextBlob('I love riding public transit')\n",
    "print(sentence.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = TextBlob('My bus was late AGAIN today.')\n",
    "print(sentence.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24086578",
   "metadata": {},
   "source": [
    "We can access the polarity score directly. (Let's ignore subjectivity.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c9aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence.sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5814ea",
   "metadata": {},
   "source": [
    "Now let's come back to our reddit comments. We can compute the sentiment (polarity) score for each comment. \n",
    "\n",
    "Let's use a list comprehension to loop over each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4320c",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# the list comprehension is the same as\n",
    "la_sentiment = []\n",
    "for c in la_metro:\n",
    "    la_sentiment.append(TextBlob(c).sentiment.polarity)\n",
    "\n",
    "la_sentiment = [TextBlob(c).sentiment.polarity for c in la_metro]\n",
    "bart_sentiment = [TextBlob(c).sentiment.polarity for c in bart]\n",
    "nyc_sentiment = [TextBlob(c).sentiment.polarity for c in nyc_rail]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f45de4b",
   "metadata": {},
   "source": [
    "We get lists of sentiments. Let's look at the first few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a32b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print('Sentiment: {:.2f}. Comment: {}'.format(la_sentiment[i], la_metro[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911c99f5",
   "metadata": {},
   "source": [
    "Now let's visualize it. A histogram seems appropriate here.\n",
    "\n",
    "Note that the seaborn `histplot` can take a list or any other sequence, as well as a `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3963a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "help(sns.histplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d530348",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(la_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00157e39",
   "metadata": {},
   "source": [
    "Let's compare the different transit agencies.\n",
    "\n",
    "We've already used the `plt.subplots()` function to create a set of axes. However, its real power comes in creating figures with multiple plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab453e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# this creates a 1x3 matrix of plots, and returns a list of axes objects\n",
    "fig, axes = plt.subplots(1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a98491",
   "metadata": {},
   "source": [
    "Because `axes` is a list, we can now access each axis as `axes[0]`, `axes[1]`, etc.\n",
    "\n",
    "We can even loop over each axis as with any other list. That's useful for plotting lots of subgroups.\n",
    "\n",
    "Here, we'll use a loop to clean up each plot. We'll use the `zip` notation, that loops over equal-length lists and pairs them up. For example, the first iteration of the loop will put the first element of `axes` in `ax`, and the first element of `cities` in `city`.\n",
    "\n",
    "We'll make the axes have the same extent for each plot, and set the y-axis label on only the left-hand axis.\n",
    "\n",
    "`fig.tight_layout()` is a useful command to clean up the spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91027146",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3)\n",
    "\n",
    "sns.histplot(la_sentiment, ax=axes[0])\n",
    "sns.histplot(bart_sentiment, ax=axes[1])\n",
    "sns.histplot(nyc_sentiment, ax=axes[2])\n",
    "\n",
    "subreddits = ['LA Metro', 'BART', 'NYC Rail']\n",
    "for ax, sr in zip(axes, subreddits):\n",
    "    ax.set_title(sr)\n",
    "    ax.set_ylim(0,50)\n",
    "    ax.set_xlim(-1,1)\n",
    "    ax.set_ylabel('')\n",
    "axes[0].set_ylabel('Number of comments')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc4a484",
   "metadata": {},
   "source": [
    "There is lots more we can do here. For example:\n",
    "* We relied on different subreddits, which might attract different types of users. Perhaps it makes more sense to look at a broader city subreddit, and then filter for posts about buses and trains.\n",
    "* We could tokenize (split) each comment into sentences. Otherwise, for longer comments, the more polarized (opinionated) comments might be watered down with other sentences.\n",
    "* We could use a different sentiment analyzer ([`TextBlob` has a couple of pre-trained options](https://textblob.readthedocs.io/en/dev/advanced_usage.html)) or train our own sentiment analyzer using the `nltk` \n",
    "functionality. \n",
    "* Note that sentiment analyzers are often trained on movie reviews and similarly \"opinionated\" corpuses, and so more specialist applications need custom training. In some of my [own work](https://conbio.onlinelibrary.wiley.com/doi/10.1111/csp2.624), we found that the writing was too technical or dry in style for sentiment analysis to work.\n",
    "\n",
    "But I'll leave those for you to explore on your own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ce3eca",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "    <li>Sentiment analysis can identify positive and negative sentiments towards a topic. The pre-trained models might not work well for your data, but you can train your own.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
