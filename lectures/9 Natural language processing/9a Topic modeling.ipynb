{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7fce1b",
   "metadata": {},
   "source": [
    "# Module 9. Natural language processing\n",
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd890dd",
   "metadata": {},
   "source": [
    "## Lecture objectives\n",
    "1. Provide practice with importing PDFs and handling exceptions\n",
    "2. Demonstrate how to estimate a topic model using LDA\n",
    "3. Discuss how to adjust the model parameters in the search for a meaningful topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a58180",
   "metadata": {},
   "source": [
    "So far, we've got some text into Python from a PDF, cleaned it up, and split the string into sentences and words. We've also done simple word counts. Now, we'll see what topic modeling - one method under the broader umbrella of natural language processing - can do.\n",
    "\n",
    "Two examples in urban studies that use topic modeling are [Han et al.](https://www.tandfonline.com/doi/full/10.1080/01944363.2020.1831401) who analyze election communications, and [Brinkley & Stahmer](https://journals.sagepub.com/doi/abs/10.1177/0739456X21995890) who analyze General Plans. But let's say a little about the principles first.\n",
    "\n",
    "Topic modeling is a method to identify patterns in text documents. It's an *unsupervised* machine learning technique, i.e. it doesn't need to be \"trained\" on a dataset of known topics. Rather, it identifies topics from the ground up. \n",
    "\n",
    "In that sense, topic modeling is related to cluster analysis. However, a document can belong (partially) to multiple topics, where an observation is typically assigned to a single cluster.\n",
    "\n",
    "But both cluster analysis and topic modeling perform *dimensionality reduction*. Suppose we have 1000 distinct words in a document, that we reduce to 10 topics. We are going from a 1000-dimensional space to a 10-dimensional space. Obviously, 10 dimensions are much easier to interpret.\n",
    "\n",
    "There are several algorithms that implement topic modeling; we'll focus on Latent Dirichlet Allocation (LDA) (as in the Han et al. paper). We'll use the `gensim` library, although `sklearn` is a popular alternative. `gensim` is dedicated to NLP, while `sklearn` is a more general machine-learning library.\n",
    "\n",
    "We'll start with looking at a selection of Climate Action Plans (CAPs) from cities and counties in California. These are some of the plans I analyzed [in a project on equity in CAPS](https://journals.sagepub.com/doi/10.1177/0739456X211072527) with Hillary Angelo, Key MacFarlane, and James Sirigotis. Thanks to these collaborators for permission to share this dataset.\n",
    "\n",
    "The plans are in a directory in your GitHub repository. We can see the list using `os`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce89b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "filelist = os.listdir('../data/CAPs')  \n",
    "\n",
    "# Look at the first 10\n",
    "filelist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d29540",
   "metadata": {},
   "source": [
    "Rather than writing code to read each file individually, let's write a function which takes a filename, and returns the text of that file. We've seen all this code before.\n",
    "\n",
    "We can then create a list of documents. Each element of that list will be the text of a single CAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2409acc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import re\n",
    "\n",
    "def readPDF(filename):\n",
    "    txt = extract_text('../data/CAPs/'+filename)\n",
    "    \n",
    "    # use regex to remove punctuation, numbers, etc.\n",
    "    txt = re.sub(r\"[^A-z\\s]\", \"\", txt)\n",
    "    # and to remove whitepace\n",
    "    txt = re.sub(r\"\\s+\", \" \", txt) \n",
    "    \n",
    "    print('Finished {}'.format(filename))\n",
    "    return txt\n",
    "\n",
    "# use a list comprehension to read in all the files\n",
    "# only do those that end with .pdf\n",
    "caps = [readPDF(fn) for fn in filelist if fn.endswith('.pdf')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0f747",
   "metadata": {},
   "source": [
    "Let's look at some random parts of a couple of plans to make sure they loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb23199",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(caps[0][10000:10200])\n",
    "print(caps[2][10000:10200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37caa953",
   "metadata": {},
   "source": [
    "Now, as before, let's remove the stop words. \n",
    "\n",
    "We'll use a list comprehension again, but this time in a nested way.  In the outer list, we loop over plans. In the inner list, we loop over words in that plan.\n",
    "\n",
    "We could have done this another way through adding to the `readPDF` function, and making that return a list of words that exclude stop words.\n",
    "\n",
    "At this point, we might want to lemmatize as well, but we'll skip that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593d413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "swords = stopwords.words('english')\n",
    "wordlists = [[word for word in word_tokenize(cap.lower()) \n",
    "                 if word not in swords] \n",
    "                 for cap in caps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eaab70",
   "metadata": {},
   "source": [
    "So for each plan, we have a list of words. For example, the first ten words from the first plan are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550e4465",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordlists[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520b8cd",
   "metadata": {},
   "source": [
    "Which makes sense when we look at the start of that plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca56e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "caps[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44be194f",
   "metadata": {},
   "source": [
    "We are at the point where we can do topic modeling.\n",
    "\n",
    "The `gensim` documentation is pretty thorough.\n",
    "\n",
    "Note that there are lots of options. The most important are:\n",
    "* `corpus`: the text. More on the format for this below\n",
    "* `num_topics`: how many topics you want to identify\n",
    "* `alpha`: the expected distribution of topics across documents (i.e., are topics concentrated in a few documents)\n",
    "* `eta`:  (sometimes called `beta`): the expected distribution of words across topics (i.e., are words concentrated in a few topics) \n",
    "\n",
    "However, sensible defaults are provided. So normally, a good approach is to start with the defaults and adjust accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fe4e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "help(gensim.models.LdaMulticore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b5cbb3",
   "metadata": {},
   "source": [
    "[The `gensim` documentation](https://radimrehurek.com/gensim/models/ldamulticore.html) provides some simple examples that help with getting up and running. There's also many other examples online. \n",
    "\n",
    "Conceptually, we need to:\n",
    "* Convert our text to a list of lists. The outer list is each plan, and the inner list is words in that plan. We did this already.\n",
    "* Create a gensim Dictionary (basically, each word gets an integer id)\n",
    "* Feed that to the `gensim.models.LdaMulticore` function\n",
    "\n",
    "The tricky part here is getting the data into the format (e.g. a list of words, list of word ids, string) that `gensim` expects. I did this mostly by adapting the examples on the `gensim` website.\n",
    "\n",
    "Here, we choosing 5 topics - an arbitrary number. We aren't specifying the `alpha` and `eta` hyperparameters. Rather, we are using the `gensim` defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5b0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(wordlists)\n",
    "corpus = [dictionary.doc2bow(wl) for wl in wordlists]\n",
    "# LdaMulticore uses multiple cores (thus, it runs faster)\n",
    "# If you have problems, try replacing LdaMulticore with LdaModel\n",
    "model = gensim.models.LdaMulticore(corpus, id2word=dictionary, num_topics=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aace8d9",
   "metadata": {},
   "source": [
    "The `model` object now contains the results of our LDA model. We can explore some of its attributes and functions. The [documentation](https://radimrehurek.com/gensim/models/LdaModel.html#module-gensim.models.LdaModel) is also fairly comprehensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee39d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1653db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.show_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eaac88",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "So it looks like we have some topics. But there is a lot of nonsense. \n",
    "\n",
    "First of all, there are some one and two letter words.\n",
    "\n",
    "Second, what is `cidcid`? \n",
    "\n",
    "Let's troubleshoot. We can find a plan that has this string in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed0974",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cap in caps:\n",
    "    if 'cidcid' in cap:\n",
    "        # stop once we find that\n",
    "        break\n",
    "        \n",
    "# look at the start of that plan\n",
    "cap[:6000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc81a9",
   "metadata": {},
   "source": [
    "Who knows what caused that. PDFs are hard to parse!\n",
    "\n",
    "So let's drop everything from our wordlist that is:\n",
    "* Less than 3 letters\n",
    "* Has `cid` in it\n",
    "\n",
    "We can add these conditions. Note that we check separately to see if the word is equal to `cid`, or if multiple cids are in the word. This avoids dropping words like \"incidental.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c2cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "swords = stopwords.words('english')\n",
    "wordlists = [[word for word in word_tokenize(cap.lower()) \n",
    "                 if word not in swords and len(word)>2 and word!='cid' and 'cidcid' not in word] \n",
    "                 for cap in caps]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a18b357",
   "metadata": {},
   "source": [
    "And estimate our topic model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c4262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(wordlists)\n",
    "corpus = [dictionary.doc2bow(wl) for wl in wordlists]\n",
    "model = gensim.models.LdaMulticore(corpus, id2word=dictionary, num_topics=5)\n",
    "model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cfdd37",
   "metadata": {},
   "source": [
    "This looks better! \n",
    "\n",
    "## Visualizing topic models\n",
    "How can we visualize the topics?\n",
    "\n",
    "The data format looks pretty straightforward. Each topic is a list of tuples of (word, weight). With some effort, we might be able to plot this ourselves. \n",
    "\n",
    "But a quick web search reveals that there is a [Python library specificially designed to visualize LDA outputs](https://pyldavis.readthedocs.io/en/latest/readme.html)! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dda3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models   # note that in previous versions this was called pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.gensim_models.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f830ee",
   "metadata": {},
   "source": [
    "How do we interpret the visualization? You can find a detailed explanation [here](https://aclanthology.org/W14-3110.pdf). In short:\n",
    "* The size of each circle is proportional to the frequency of that topic\n",
    "* The distance between circles shows how closely they are related\n",
    "* The bars represent the overall frequency of a word (blue) and the frequency within that topic\n",
    "* The \"relevance metric\" slider controls the sorting. The default is to sort by overall frequency within a topic. As you slide to the left, words that are disproportionately frequent in that topic rise to the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700ec179",
   "metadata": {},
   "source": [
    "## Making sense of topic models\n",
    "So what does all this mean?\n",
    "\n",
    "This is where exploratory analysis comes in. We probably want to adjust the number of topics and the parameters, until we find a set of topics that makes intuitive sense.\n",
    "\n",
    "If we were doing regression analysis, this would be \"fishing\" (bad!). But in unsupervised machine learning, this type of exploration is an inherent part of the process. We aren't testing hypotheses, just searching for patterns and understanding.\n",
    "\n",
    "Remember that the `LdaMulticore` function takes the `num_topics`, `alpha` and `eta` parameters. We could adjust those. We might also want to exclude certain uninformative words based on our present context â€” perhaps \"city\" or \"climate.\"\n",
    "\n",
    "`alpha` controls the expected distribution of topics across documents. A higher value of `alpha` means that each document is expected to contain more of a mix of topics, rather than focusing on a few topics.\n",
    "\n",
    "`eta` (sometimes called beta) controls the expected distribution of words across topics. A higher value of `eta` means that topics are more similar in terms of their mixture of words.\n",
    "\n",
    "For climate plans, let's assume that documents have more of a mix of topics, so we'll set `alpha=0.9`. For `eta`, it's unclear what to expect, but let's try `eta=0.5` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_exclude = ['city','climate','action','emissions','ghg','reduction','plan']\n",
    "\n",
    "wordlists = [[word for word in word_tokenize(cap.lower()) \n",
    "                 if word not in swords and word not in words_to_exclude\n",
    "                     and len(word)>2 and word!='cid' and 'cidcid' not in word] \n",
    "                 for cap in caps]\n",
    "dictionary = gensim.corpora.Dictionary(wordlists)\n",
    "corpus = [dictionary.doc2bow(wl) for wl in wordlists]\n",
    "\n",
    "model = gensim.models.LdaMulticore(corpus, id2word=dictionary, num_topics=5, alpha = 0.7, eta=0.5)\n",
    "pyLDAvis.gensim_models.prepare(model, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba198e2",
   "metadata": {},
   "source": [
    "That gave some insights into what types of topics were included in plans, but it was hard to find a meaningful group of topics. We could experiment further with the hyperparameters. But it could also be a limitation of the small number of plans (30) in our sample. With a few hundred plans, we might generate further insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf819dad",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Experiment with the parameters (and perhaps the list of words that you exclude), and see if you can come up with sensible topics.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffaf321",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>Topic modeling is a form of dimensionality reduction. The aim is to make your data easier to interpret.</li>\n",
    "  <li>Exploration and iteration are the keys.</li>\n",
    "  <li>After your first set of results, maybe there are texts or words that you want to exclude.</li>\n",
    "  <li>Then, experiment with adjusting the parameters.</li>\n",
    "    <li>Success is defined based on whether you find the results useful!</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
