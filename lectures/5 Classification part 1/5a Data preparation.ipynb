{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4995c060",
   "metadata": {},
   "source": [
    "# Module 5. Classification part 1\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "## Lecture objectives\n",
    "\n",
    "1. Introduce the principles of machine learning\n",
    "2. Provide more practice with data wrangling\n",
    "\n",
    "Machine learning is a very general term that covers many parts of data science. Here, we will look at two specific problems that machine learning is well equipped to handle:\n",
    "* Classification (this module and the next)\n",
    "* Clustering (the subsequent module)\n",
    "\n",
    "As a broad generalization, machine learning-based classification focuses on *prediction*. For example: [which neighborhoods are likely to gentrify](https://journals.sagepub.com/doi/abs/10.1177/0042098018789054)? [Which facilities are likely to be violating environmental standards?](https://www.nature.com/articles/s41893-018-0142-9) What is demand likely to be at a new bikeshare station? [What is the race and gender of an author on a course reading list](http://syllabusdiversity.org)?\n",
    "\n",
    "There are also applications which raise more concerns with ethics and justice (yes, [predictive policing](https://www.technologyreview.com/2020/07/17/1005396/predictive-policing-algorithms-racist-dismantled-machine-learning-bias-criminal-justice/), I'm talking about you). We'll come back to these issues in a couple of weeks.\n",
    "\n",
    "Machine learning is less successful with questions of *causation* and *hypothesis testing*. Here, a statistical approach (frequentist or Bayesian) is likely to be more appropriate, although there is quite a bit of overlap between \"statistics\" and \"machine learning.\"\n",
    "\n",
    "There are at least three widely used approaches to classification.\n",
    "* Logistic regression. This is often used in a more statistical setting, but is the starting point for much machine learning analysis. \n",
    "* Random forests. We'll focus on this technique.\n",
    "* Neural networks. Often used for image recognition, this can be a \"black box\" approach to prediction and classification.\n",
    "\n",
    "Important: machine learning is a very large field, and there are entire courses on the theory and applications. Here, we will give a very high-level overview. We'll focus on the big-picture applicability of machine learning techniques, and actually implementing them in Python. We'll skate over the theoretical underpinnings and the details of the various algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6ef63",
   "metadata": {},
   "source": [
    "## Example: ADUs in LA\n",
    "The example we will use is whether property owners construct Accessory Dwelling Units (ADUs) in the City of Los Angeles. You might imagine that a predictive approach could be useful to planners and policymakers. Not least, they could predict future ADU growth, and the neighborhoods where ADUs are most likely to be built.\n",
    "\n",
    "We can obtain the data from the City's building permits database (which tells us whether or not an ADU was built), and the County Assessor parcel database (which provides covariates such as lot size). Because both of these datasets are very large, I preprocessed them and saved a slimmed-down version that is in your GitHub folder. Specifically, I extracted a subset of fields, limited the building permits to those that include an ADU, and limited the parcels to those in the City of LA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1637078",
   "metadata": {},
   "source": [
    "## Wrangling the data\n",
    "We have two input data files: permits and parcels. The aim: add a column to the parcels dataframe that is `True` if an ADU has been permitted on that parcel, and `False` otherwise.\n",
    "    \n",
    "Even with this preprocessing, there is some work to do in joining the datasets together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e13cb46",
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# get building permit data\n",
    "# this is an abbreviated version of the data here (>500 MB):\n",
    "# https://data.lacity.org/City-Infrastructure-Service-Requests/Building-and-Safety-Permit-Information-Old/yv23-pmwf\n",
    "\n",
    "# this code was used to read in the data and save a subset (ADU permits only)\n",
    "# that is manageable in size\n",
    "if 0:  # if 0 means this block won't be executed (because 0 is False)\n",
    "    cols_to_use = ['Assessor Book', 'Assessor Page', 'Assessor Parcel', '# of Accessory Dwelling Units']\n",
    "    df = pd.read_csv('Building_and_Safety_Permit_Information_Old.csv', usecols=cols_to_use)\n",
    "    df = df[df['# of Accessory Dwelling Units']>0]\n",
    "    df.to_csv('ADU_permits.csv', index=False)\n",
    "\n",
    "permits = pd.read_csv('../data/ADUs/ADU_permits.csv')  # this file should be in your GitHub folder\n",
    "permits.head()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c50b4d",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# original data: https://egis-lacounty.hub.arcgis.com/datasets/parcels\n",
    "# this code was used to read in the data and save a subset \n",
    "# (City of LA only, subset of columns) that is manageable in size\n",
    "\n",
    "import geopandas as gpd\n",
    "\n",
    "if 0: # if 0 means this block won't be executed\n",
    "    gdf = gpd.read_file('LACounty_Parcels.gdb', driver='FileGDB', layer='LACounty_Parcels')\n",
    "    gdf.dropna(subset=['SitusCity'], inplace=True)\n",
    "    gdf = gdf[gdf['SitusCity'].str.startswith('LOS ANGELE')]\n",
    "    cols_to_use = ['APN', 'UseType', 'UseDescription','YearBuilt1', 'Units1','Bedrooms1', 'Bathrooms1', \n",
    "         'SQFTmain1','Roll_LandValue', 'Roll_ImpValue', 'Roll_LandBaseYear', 'Roll_ImpBaseYear', 'CENTER_LAT', 'CENTER_LON']\n",
    "    parceldf = pd.DataFrame(gdf)[cols_to_use]  # drops the geometry column as well\n",
    "    parceldf.to_csv('parcels.csv', index=False)\n",
    "    del gdf   # frees up space\n",
    "\n",
    "parcels = pd.read_csv('../data/ADUs/parcels.csv')\n",
    "parcels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3d9527",
   "metadata": {},
   "source": [
    "Note that the `APN` column in `parcels` has a format that corresponds to three columns in `permits`: `Assessor Book`-`Assessor Page`-`Assessor Parcel`. \n",
    "\n",
    "So the first step is to create this `APN` column in `permits`.\n",
    "\n",
    "We first convert each column to an integer (to drop the decimal point), then to a string, then pad with zeros, and then concatenate the columns separated by `-`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a2a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join\n",
    "permits['APN'] = (permits['Assessor Book'].astype(int).astype(str).str.zfill(4) + '-' \n",
    "                   + permits['Assessor Page'].astype(int).astype(str).str.zfill(3) + '-'\n",
    "                   + permits['Assessor Parcel'].astype(int).astype(str).str.zfill(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e795b1b8",
   "metadata": {},
   "source": [
    "What happened? Note two things:\n",
    "* The problem is that we are trying to convert `'***'` to an integer\n",
    "* The error is being caused by the `permits['Assessor Parcel'].astype(int)` part of the code.\n",
    "\n",
    "So let's look at those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9060de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "permits[permits['Assessor Parcel']=='***'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf1b00a",
   "metadata": {},
   "source": [
    "It seems like the parcel number is just missing, so let's drop them.\n",
    "\n",
    "Note the `!=` operator means \"not equal to.\" So we are keeping the rows that are *not* `***`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc17b6",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "permits = permits[permits['Assessor Parcel']!='***']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df756fb",
   "metadata": {},
   "source": [
    "Now let's try to create the column again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea9fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "permits['APN'] = (permits['Assessor Book'].astype(int).astype(str).str.zfill(4) + '-' \n",
    "                   + permits['Assessor Page'].astype(int).astype(str).str.zfill(3) + '-'\n",
    "                   + permits['Assessor Parcel'].astype(int).astype(str).str.zfill(3))\n",
    "permits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d560429",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Question:</strong> What type of join do we want? Left? Right? Inner? Outer? 1:1? 1:many?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223f7c9",
   "metadata": {},
   "source": [
    "Note two things:\n",
    "* We need to keep all of the parcels, even if there isn't a corresponding permit. Otherwise, we can't do any predictionâ€”we'd have a dataset where *every* parcel has an ADU. So that implies a left join to the parcels dataframe\n",
    "* We don't want to duplicate parcels. So let's drop any duplicates (on the APN column) in both the permit and parcels dataframes. That will guarantee a 1:1 join\n",
    "\n",
    "Let's first check to see if duplicates exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4e31dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "permits.APN.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8a5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels.APN.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46bb698",
   "metadata": {},
   "source": [
    "There are two ways to drop duplicates: the [pandas `drop_duplicates()` function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html) is one.\n",
    "\n",
    "But sometimes it's easier to use `groupby`, and then take the first in each group. If there is only one row in a group, it will be returned unchanged.\n",
    "\n",
    "A byproduct of using `groupby` on the `APN` column is that `APN` is now our index. That will make the join easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the permits, take the first row of any duplicates for convenience\n",
    "print('Before dropping duplicates: {}'.format(len(permits)))\n",
    "permits = permits.groupby('APN').first()\n",
    "print('After dropping duplicates: {}'.format(len(permits)))\n",
    "permits.index.is_unique  # make sure the index (APN) is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf92d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "permits.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f767a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before dropping duplicates: {}'.format(len(parcels)))\n",
    "parcels = parcels.groupby('APN').first()\n",
    "print('After dropping duplicates: {}'.format(len(parcels)))\n",
    "parcels.index.is_unique  # make sure the index (APN) is unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be1412",
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15a248",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> How would you join the permits dataframe to the parcels?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2cc88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDf = parcels.join(permits, how='left') # left is the default so we could omit that argument\n",
    "print('N parcels: {}'.format(len(joinedDf)))\n",
    "print('N joined: {}'.format(joinedDf['# of Accessory Dwelling Units'].count()))\n",
    "joinedDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a167ac26",
   "metadata": {},
   "source": [
    "That seems good enough. We join almost all of the permits to the parcels dataframe. \n",
    "\n",
    "Now let's create a column that is 0 if there is no ADU (i.e., if the permit data did not join), and 1 otherwise.\n",
    "\n",
    "We'll use our `lambda` function again. If the value of the column is Null (using the handy `pd.isnull`), we'll return `False`. Otherwise, `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95528631",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDf['hasADU'] = joinedDf['# of Accessory Dwelling Units'].apply(\n",
    "                        lambda x: False if pd.isnull(x) else True)\n",
    "joinedDf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfcea6e-d773-4c23-bf31-e1b68609d589",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Rewrite the above line of code using a regular function rather than an anonymous (lambda) function. Verify you get the same result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c7d0e1-1baf-4607-8a54-7971f48f14e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasADU(n_ADUs):\n",
    "    if pd.isnull(n_ADUs):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "joinedDf['hasADU2'] = joinedDf['# of Accessory Dwelling Units'].apply(hasADU)\n",
    "\n",
    "joinedDf.hasADU2 == joinedDf.hasADU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5306c381-d356-4b16-ab23-ac87dc43fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.all verifies that all elements of an array are True\n",
    "import numpy as np\n",
    "print(np.all(joinedDf.hasADU2 == joinedDf.hasADU))\n",
    "\n",
    "# delete the temporary column\n",
    "joinedDf.drop(columns=['hasADU2'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c022aec5",
   "metadata": {},
   "source": [
    "Let's stop there for now. We'll save the data so that we can reload it at the start of the next video lecture.\n",
    "\n",
    "You could save it as a `csv`. But we can also save the pandas DataFrame object, through \"pickling\" it. This is convenient when you want to save something temporarily, but it's not advisable for long-term archiving or sharing your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "joinedDf.to_pickle('../scratch/joined_permits.pandas')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93785bc8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>Machine learning is particularly valuable for prediction, and when there are many highly correlated variables.</li>\n",
    "  <li>Data wrangling is almost always your first step, and joins will come with practice.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
