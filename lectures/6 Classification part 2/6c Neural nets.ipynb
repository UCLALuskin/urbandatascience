{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4995c060",
   "metadata": {},
   "source": [
    "# Neural networks and logistic regression\n",
    "## Lecture objectives\n",
    "\n",
    "1. Demonstrate how to estimate neural network models\n",
    "2. Demonstrate how to estimate logistic regression models\n",
    "3. Provide more practice with train-test splits and assessing model performance\n",
    "4. Show how to standardize data\n",
    "\n",
    "In the previous video lectures, we estimated a random forests model using `scikit-learn`. \n",
    "\n",
    "Here, we'll explore other machine learning algorithms.\n",
    "\n",
    "Most have almost identical syntax, meaning once you are familiar with one model it's easy to apply another model. However, they will have different hyperparameters, such as the number of trees in the random forest.\n",
    "\n",
    "To start with, let's do the following:\n",
    "* load the data we previous saved as a pickle\n",
    "* recreate the dummy variables\n",
    "* create a dataframe with the subset of variables that we want to use, and drop the NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the same code as from the previous lecture\n",
    "import pandas as pd\n",
    "joinedDf = pd.read_pickle('../scratch/joined_permits.pandas')\n",
    "\n",
    "dummies1 = pd.get_dummies(joinedDf.UseType, prefix='usetype_')  # creates a dataframe of dummies\n",
    "dummies2 = pd.get_dummies(joinedDf.UseDescription, prefix='usedesc_')\n",
    "joinedDf = joinedDf.join(dummies1).join(dummies2) \n",
    "\n",
    "xvars = (dummies1.columns.tolist() + dummies2.columns.tolist() + \n",
    "            ['YearBuilt1', 'Units1', 'Bedrooms1', 'Bathrooms1', 'SQFTmain1', \n",
    "             'Roll_LandValue', 'Roll_ImpValue', 'Roll_LandBaseYear', \n",
    "             'Roll_ImpBaseYear', 'CENTER_LAT', 'CENTER_LON' ])\n",
    "yvar = 'hasADU'\n",
    "\n",
    "# create a dataframe with no NaNs\n",
    "df_to_fit = joinedDf[xvars+[yvar]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6e94cd",
   "metadata": {},
   "source": [
    "Let's also import the relevant `scikit-learn` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e011e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd3f0a",
   "metadata": {},
   "source": [
    "## Standardizing data\n",
    "Many machine learning algorithms are more robust if we *standardize* the data - subtract the mean and divide by the standard deviation. This puts each variable on a common scale.\n",
    "\n",
    "It didn't matter for random forests, but it does for neural networks.\n",
    "\n",
    "Let's do this. Note that we need to exclude the dummy variable columns and the dependent variable.\n",
    "\n",
    "To identify them, we'll use a Python [list comprehension](https://www.w3schools.com/python/python_lists_comprehension.asp). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: add a suffix to each column\n",
    "oldcols = ['units', 'squarefeet','lotsize']\n",
    "newcols = []\n",
    "for col in oldcols:\n",
    "    newcols.append(col+'_2020')\n",
    "print(newcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc2bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is much cleaner as a list comprehension\n",
    "[col+'_2020' for col in oldcols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd01524",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = [col for col in df_to_fit.columns \n",
    "                if col.startswith('usetype_') or col.startswith('usedesc_') \n",
    "                or col=='hasADU']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde40976-ed12-4bcf-bcd5-7dd20498438c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> How would you rewrite this list comprehension as a <strong>for</strong> loop?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0781cc-2b85-4a2a-a0c1-5f8294fc1822",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = []\n",
    "for col in df_to_fit.columns:\n",
    "    if col.startswith('usetype_') or col.startswith('usedesc_') or col=='hasADU':\n",
    "        cols_to_exclude.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b55ab5",
   "metadata": {},
   "source": [
    "Now let's create a list of the columns that we *don't* want to exclude. \n",
    "\n",
    "We can use a list comprehension again: we'll include that column in the new list if the condition (`col not in cols_to_exclude`) is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ee4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "otherCols = [col for col in df_to_fit.columns if col not in cols_to_exclude]\n",
    "otherCols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385a31d",
   "metadata": {},
   "source": [
    "Now let's scale `otherCols`. Note that the `StandardScaler` returns a numpy array, not a pandas DataFrame. So we need to convert the array to a dataframe and specify the column names and the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4105a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://scikit-learn.org/stable/modules/preprocessing.html for standardization\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(df_to_fit[otherCols])\n",
    "\n",
    "# convert to DataFrame and specify the column names and index\n",
    "df_scaled = pd.DataFrame(scaler.transform(df_to_fit[otherCols]), \n",
    "                         columns=otherCols, index=df_to_fit.index)\n",
    "\n",
    "# create a DataFrame with these scaled columns joined to the columns that we didn't scale\n",
    "df_scaled = df_scaled.join(df_to_fit[cols_to_exclude])\n",
    "\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7139839f-6714-4d29-a9a2-af605f792f76",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Exercise:</strong> Verify that the standardization works (each column should have mean zero and standard deviation one).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba33fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef008d8",
   "metadata": {},
   "source": [
    "We'll do our train/test split as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae29fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "      df_scaled[xvars], df_scaled[yvar], test_size = 0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33657598",
   "metadata": {},
   "source": [
    "And estimate our neural network model. \n",
    "\n",
    "Note that the workflow and syntax is very similar to the random forests:\n",
    "* Initialize the classifier object - here, we call it `mlp`\n",
    "* Fit to the data\n",
    "* Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02889322",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "y_pred = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94361499",
   "metadata": {},
   "source": [
    "How did we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d450b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e65db07",
   "metadata": {},
   "source": [
    "Interestingly, we get very similar results to the random forests. Perhaps this indicates the inherent unpredictability of ADUs, given how rarely they are constructed. Or we might be able to do better with additional predictors or through adjusting the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fd413",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "As a point of comparison, how would a more traditional logistic regression fare?\n",
    "\n",
    "Many different regression estimators are implemented in `scikit-learn`. And the syntax should be familiar by now. Note that standardization (as we did for neural networks) helps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61237a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cef0fc",
   "metadata": {},
   "source": [
    "Note that it doesn't even converge! Methods like logistic regression don't handle highly correlated variables very well.\n",
    "\n",
    "We might be able to do better with a smaller set of predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6120ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xvars = ['YearBuilt1', 'Units1', 'Bedrooms1', 'Bathrooms1', 'SQFTmain1', 'Roll_LandValue', \n",
    "             'Roll_ImpValue', 'Roll_LandBaseYear', 'Roll_ImpBaseYear', 'CENTER_LAT', 'CENTER_LON', 'usedesc__Single']\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train[xvars], y_train)\n",
    "y_pred = lr.predict(X_test[xvars])\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c65f60",
   "metadata": {},
   "source": [
    "Not so great, eh? So our random forests and neural networks approaches look much better by comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93785bc8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>There are several different approaches to machine learning. Random forests and neural networks are two of the most popular.</li>\n",
    "  <li>scikit-learn provides a consistent syntax: initialize-fit-predict. So once you've done one ML model, others are much simpler.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
