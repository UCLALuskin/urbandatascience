{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545b3a7a",
   "metadata": {},
   "source": [
    "# Module 7. Clustering\n",
    "# Introduction to clustering\n",
    "## Lecture objectives\n",
    "1. Examine the purposes and potential uses of clustering\n",
    "2. Introduce more exploratory data analysis techniques, such as pairplots\n",
    "3. Demonstrate how to implement k-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e226ed",
   "metadata": {},
   "source": [
    "## Why cluster?\n",
    "Cluster analysis is an exploratory technique to identify sensible groupings in a dataset. The analyst has no prior knowledge of what these clusters are, and the data are not labeled with the \"correct\" cluster. Thus, cluster analysis is an *unsupervised* machine learning technique.\n",
    "\n",
    "Some potential applications of clustering:\n",
    "* Identify types of Marine Protected Area (e.g., [Bohorquez et al. 2019](https://www.sciencedirect.com/science/article/pii/S0308597X19304439))\n",
    "* Identify types of street networks (e.g., [Barrington-Leigh and Millard-Ball 2020](https://www.pnas.org/content/117/4/1941))\n",
    "* Identify types of neighborhood (e.g., [Kendig 2007](https://www.tandfonline.com/doi/abs/10.1080/01944367608977731))\n",
    "* Identify types of transit agencies (e.g. [Ederer et al. 2019](https://journals.sagepub.com/doi/full/10.1177/0361198119852074))\n",
    "* Identify patterns of cruising for parking (e.g. [Millard-Ball, Weinberger & Hampshire 2021](https://findingspress.org/article/28061-the-shape-of-cruising))\n",
    "\n",
    "Clustering the data in this way can help you see regularities in the data that you can then interpret. It might suggest policies that are appropriate for one group of cities or agencies but not another. Or it could identify a peer group against which to benchmark (say) affordable housing construction costs or transit on-time performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e7b24",
   "metadata": {},
   "source": [
    "## Types of clustering\n",
    "Formally, clustering takes a set of *N* objects and finds *K* groups based on a measure of similarity. For a technical yet accessible overview, I recommend [Jain 2010](https://www.sciencedirect.com/science/article/abs/pii/S0167865509002323). \n",
    "\n",
    "The two broad groups of clustering algorithms are *hierarchical* and *partitional*. \n",
    "\n",
    "Ederer et al. (2019), for example, use a hierarchical algorithm to classify transit agencies.\n",
    "\n",
    "<img src=\"https://journals.sagepub.com/na101/home/literatum/publisher/sage/journals/content/trra/2019/trra_2673_11/0361198119852074/20191218/images/medium/10.1177_0361198119852074-fig1.gif\" style=\"width:50%;\"/>\n",
    "\n",
    "\n",
    "But let's start with a partitional algorithm. The most popular is called *k-means*. Again, this is an exploratory data analysis process. The analyst needs to specify the number of clusters *K*, and should experiment with different values of *K* until a meaningful grouping emerges. Another way to choose *K* is the [elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)), but we won't discuss that here.\n",
    "\n",
    "## Example: precinct-level voting\n",
    "We'll use the `sklearn` library to implement the k-means algorithm. The aim: identify a typology of voters based on precinct-level data.\n",
    "\n",
    "The California [Statewide Database](https://statewidedatabase.org), maintained by UC Berkeley, provides access to voting data. Your GitHub repository should include the November 2020 precinct data for Los Angeles County."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a43ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/c037_g20_sov_data_by_g20_srprec.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b14775",
   "metadata": {},
   "source": [
    "The unique identifier is given by the precinct column, `srprec`, so let's set that as the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04a6277",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('srprec', inplace=True)\n",
    "df.index.is_unique  # verify that it's unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd528ac9",
   "metadata": {},
   "source": [
    "What columns are in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d700fa9",
   "metadata": {},
   "source": [
    "There are obviously a lot of columns. You can see the [full codebook here](https://statewidedatabase.org/d10/g20.html). But the propositions are pretty self explanatory. `PRSDEM01` gives votes for Biden, `PRSREP01` for Trump, etc. Note the state Senate and Assembly races will have different candidates depending on the precinct, so let's ignore those.\n",
    "\n",
    "What do the data in these columns look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa91f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PRSDEM01','PRSREP01']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac605a95",
   "metadata": {},
   "source": [
    "We see that the numbers are in absolute terms. Let's convert them to vote share."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31897bd8-7995-4c02-9e0d-2512a984ff30",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> How would you create a new column with percentage vote share for Biden and Trump?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ca238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is share of two-party vote (ignoring other candidates)\n",
    "df['Biden_pc'] = df.PRSDEM01 / (df.PRSDEM01+df.PRSREP01)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0a5ee5",
   "metadata": {},
   "source": [
    "Let's do the same for each proposition too. \n",
    "\n",
    "How can we get a list with the numbers of the propositions? We'll use a list comprehension to get the list of relevant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d9573",
   "metadata": {},
   "outputs": [],
   "source": [
    "props = [col for col in df.columns if col.startswith('PR_') and col.endswith('Y')]\n",
    "print(props)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3450ce7b",
   "metadata": {},
   "source": [
    "And another list comprehension to get the relevant proposition numbers. Note that these are always the 4th and 5th characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c123062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use our string indexing to get the 4th and 5th characters\n",
    "# for example\n",
    "print('PR_14_Y'[3:5])\n",
    "\n",
    "# apply this in a list comprehension\n",
    "props = [prop[3:5] for prop in props]\n",
    "print(props)\n",
    "\n",
    "# we could have done this in one go\n",
    "print([col[3:5] for col in df.columns if col.startswith('PR_') and col.endswith('Y')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5234ca6a",
   "metadata": {},
   "source": [
    "Now, let's loop over this list of propositions to calculate the vote share."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f1c62",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for prop in props:\n",
    "    df[prop+'_pc_yes'] = df['PR_'+prop+'_Y'] / (df['PR_'+prop+'_Y'] \n",
    "                                              + df['PR_'+prop+'_N'])*100\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d768265",
   "metadata": {},
   "source": [
    "### Inspect and standardize the data\n",
    "\n",
    "Number 1 rule: before applying any algorithm to your data, look at it!\n",
    "\n",
    "We could create a scatterplot matrix manually. But `seaborn` has a [nice function to do this for us](https://seaborn.pydata.org/examples/scatterplot_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8b4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e6797",
   "metadata": {},
   "source": [
    "We'll plot a subset of the columns, ignoring a few of the less critical propositions. Should stem cell research and dialysis rules really be on the ballot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all the columns with 'pc' in the name\n",
    "cols_to_plot = [col for col in df.columns if '_pc' in col]\n",
    "\n",
    "# remove those we don't want\n",
    "cols_to_plot.remove('14_pc_yes') \n",
    "cols_to_plot.remove('23_pc_yes') \n",
    "cols_to_plot.remove('24_pc_yes') \n",
    "\n",
    "# kind='reg' adds the line of best fit\n",
    "ax = sns.pairplot(df[cols_to_plot], kind='reg')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f0632",
   "metadata": {},
   "source": [
    "There are pretty strong relationships between Presidential voting and the propositions. All have a positive correlation except for Prop 20 (harsher sentencing) and Prop 22 (independent contractor status for drivers for Uber, Doordash, etc.). [A helpful reminder of the propositions is here](https://ballotpedia.org/California_2020_ballot_propositions).\n",
    "\n",
    "But there isn't a perfect relationship. Perhaps cluster analysis can reveal some groupings? In other words, do precincts cluster according to ideological \"types\"?\n",
    "\n",
    "First, it helps to pre-process the data in two ways:\n",
    "* Let's align the data so that a higher percent means more progressive. This means using the percent \"no\" for Props 20 and 22\n",
    "* We should standardize each variable to mean zero and standard deviation one. This helps ensure that the distances in multidimensional space are consistent. (Since we have a percentage measure, it won't make much difference compared to a variable like population, but it's good practice.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67de41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for prop in ['20','22']:\n",
    "    df[prop+'_pc_no'] = 100 - df[prop+'_pc_yes']\n",
    "    df.drop(columns=[prop+'_pc_yes'], inplace=True)\n",
    "\n",
    "# then rerun the same code as above\n",
    "cols_to_plot = [col for col in df.columns if '_pc' in col]\n",
    "cols_to_plot.remove('14_pc_yes') \n",
    "cols_to_plot.remove('23_pc_yes') \n",
    "cols_to_plot.remove('24_pc_yes') \n",
    "\n",
    "# see https://scikit-learn.org/stable/modules/preprocessing.html for standardization\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(df[cols_to_plot])\n",
    "\n",
    "# as in the previous lecture, \n",
    "# the scaler returns a numpy array, so we cast this as a DataFrame \n",
    "# and need to specify the column names and index\n",
    "df_scaled = pd.DataFrame(scaler.transform(df[cols_to_plot]), \n",
    "                         columns=cols_to_plot, index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d739b834-b5a5-4880-8355-8b826bafa947",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Exercise:</strong> Unpack each step in the previous lines of code, and find out what is returned by <strong>scaler</strong> and <strong>scaler.transform(df[cols_to_plot])</strong>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ed323c",
   "metadata": {},
   "source": [
    "Let's check that our data still look reasonable by rerunning the same pairplot.\n",
    "\n",
    "Notice that the y axes run from about -3 to +3. This should be true for any standardized variable, as most observations are within 3 standard deviations of the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ef07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.pairplot(df_scaled, kind='reg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220a9d0",
   "metadata": {},
   "source": [
    "### KMeans in scikit-learn\n",
    "As always, the data wrangling was a large part of our work. Now, we are ready to do the cluster analysis, which is much simpler.\n",
    "\n",
    "The documention has some useful examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77695b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "KMeans?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84adcf61",
   "metadata": {},
   "source": [
    "We can specify the number of clusters. Optionally, we can specify the random state, so that we can reproduce our work. \n",
    "\n",
    "Then we fit to our data, in this case `df_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5079d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, random_state=0).fit(df_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2387faa4",
   "metadata": {},
   "source": [
    "Here, the error message is pretty helpful. Let's drop the NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0040a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_scaled))\n",
    "df_scaled = df_scaled.dropna()\n",
    "print(len(df_scaled))\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=1).fit(df_scaled)\n",
    "print(kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf4127a",
   "metadata": {},
   "source": [
    "It's not immediately obvious what we can do with this `KMeans` object. But let's explore it. Use the tab autocomplete to see the different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045717b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d98a82f",
   "metadata": {},
   "source": [
    "The cluster centers are defined by the (standardized) value for each variable. Each observation is assigned to the cluster with the closest center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d698805d",
   "metadata": {},
   "source": [
    "Notice that the `cluster_centers_` is an array that is `K x L`, where `K` is the number of clusters and `L` is the number of variables.\n",
    "\n",
    "Here, we have 5 clusters, and we used 10 variables to define the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0ee454",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.cluster_centers_.shape)\n",
    "print(len(df_scaled.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5ac34",
   "metadata": {},
   "source": [
    "The `labels_` attribute gives the label of the cluster to which each observation (i.e., each precinct) is assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a01c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c3238",
   "metadata": {},
   "source": [
    "So there are as many labels as rows in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a0f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(kmeans.labels_.shape)\n",
    "print(len(df_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1be6d04",
   "metadata": {},
   "source": [
    "That means that we can simply add the cluster id back to our original dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afee2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled['cluster_id'] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2989c7a6",
   "metadata": {},
   "source": [
    "How large is each cluster? Note that the algorithm doesn't aim to produce equal-size groupings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed68467",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.groupby('cluster_id').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f0226",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h3>Key Takeaways</h3>\n",
    "<ul>\n",
    "  <li>Cluster analysis is an exploratory data tool</li>\n",
    "  <li>They are an example of <em>data reduction</em>â€”reducing your data to something that can be readily interpreted</li>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
